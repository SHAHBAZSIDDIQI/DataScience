{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generation of basic User Doc and Post Doc based Embedding on public Dataset_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvebFeJseqce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The purpose of the notebook is to use user doc and post doc approach to generate embedding of public data set "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6z3J-oXe11H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "import random\n",
        "import string\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXJ_1uTmft-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EziiSDUIfR9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjO2brVSfZBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert2lower(inputstr):\n",
        "    return inputstr.lower()\n",
        "\n",
        "def removeNum(inputstr):\n",
        "    return re.sub(r'\\d+','',inputstr)\n",
        "\n",
        "def removePunc(inputstr):\n",
        "    for x in inputstr.lower(): \n",
        "        if x in punctuations: \n",
        "            inputstr = inputstr.replace(x, \"\")\n",
        "    return inputstr\n",
        "\n",
        "def removeWhiteSpace(inputstr):\n",
        "    return inputstr.strip()\n",
        "\n",
        "def removeStopWordsandLemmatize(inputstr):\n",
        "    token = word_tokenize(inputstr)\n",
        "    result = [i for i in token if not i in stop_words]\n",
        "    result = [lemmatizer.lemmatize(i) for i in result]\n",
        "    return ' '.join(result)\n",
        "\n",
        "def CompletePreprocessingofWord(tweetlist):\n",
        "    l = []\n",
        "    for tweet in tweetlist:\n",
        "        clean_tweet = convert2lower(tweet)\n",
        "        clean_tweet = removeNum(clean_tweet)\n",
        "        clean_tweet = removePunc(clean_tweet)\n",
        "        clean_tweet = removeStopWordsandLemmatize(clean_tweet)\n",
        "        clean_tweet = removeWhiteSpace(clean_tweet)\n",
        "        l.append(clean_tweet)\n",
        "    return l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feHctnImgYh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def getuniquehashtags(tweetlist):\n",
        "        allhashtags = []\n",
        "        for tweet in tweetlist:\n",
        "            allhashtags.extend(re.findall(r\"#(\\w+)\",str(tweet)))\n",
        "        return list(set(allhashtags))\n",
        "    \n",
        "    def getUniqueMentionedUsers(tweetlist):\n",
        "        allmentionedUser = []\n",
        "        for tweet in tweetlist:\n",
        "            allmentionedUser.extend(re.findall(r\"@(\\w+)\",str(tweet)))\n",
        "        return list(set(allmentionedUser))\n",
        "    \n",
        "    def getAllhashtags(tweetlist):\n",
        "        allhashtags = []\n",
        "        for tweet in tweetlist:\n",
        "            allhashtags.extend(re.findall(r\"#(\\w+)\",str(tweet)))\n",
        "        return allhashtags\n",
        "    \n",
        "    def getAllMentionedUsers(tweetlist):\n",
        "        allmentionedUser = []\n",
        "        for tweet in tweetlist:\n",
        "            allmentionedUser.extend(re.findall(r\"@(\\w+)\",str(tweet)))\n",
        "        return allmentionedUser\n",
        "    \n",
        "    def RemoveTags(tweetlist):\n",
        "        clean_tweetlist = []\n",
        "        for tweet in tweetlist:\n",
        "            tweet = re.sub('@[^\\s]+','',str(tweet))\n",
        "            tweet = re.sub('#[^\\s]+','',str(tweet))\n",
        "            clean_tweetlist.append(tweet)\n",
        "        return clean_tweetlist  \n",
        "    \n",
        "    def RemoveHTTP(tweetlist):\n",
        "        l = []\n",
        "        for tweet in tweetlist:    \n",
        "            clean_tweet = re.match('(.*?)http.*?\\s?(.*?)',str(tweet))\n",
        "            if(clean_tweet):\n",
        "                l.append(clean_tweet.group(1))\n",
        "            else:\n",
        "                l.append(tweet)\n",
        "            #l.append(re.sub(r\"http:\\+\",\"\",str(tweet)))\n",
        "        return l\n",
        "    \n",
        "    def RemoveHttps(tweetlist):\n",
        "        l = []\n",
        "        for tweet in tweetlist:    \n",
        "            clean_tweet = re.match('(.*?)https.*?\\s?(.*?)',str(tweet))\n",
        "            if(clean_tweet):\n",
        "                l.append(clean_tweet.group(1))\n",
        "            else:\n",
        "                l.append(tweet)\n",
        "            #l.append(re.sub(r\"http:\\+\",\"\",str(tweet)))\n",
        "        return l\n",
        "\n",
        "    def RemoveWWW(tweetlist):\n",
        "        l = []\n",
        "        for tweet in tweetlist:    \n",
        "            clean_tweet = re.match('(.*?)www.*?\\s?(.*?)',str(tweet))\n",
        "            if(clean_tweet):\n",
        "                l.append(clean_tweet.group(1))\n",
        "            else:\n",
        "                l.append(tweet)\n",
        "            #l.append(re.sub(r\"http:\\+\",\"\",str(tweet)))\n",
        "        return l\n",
        "    \n",
        "    def DeEmojify(tweetlist):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        l = []\n",
        "        for tweet in tweetlist:\n",
        "            #l.append(emoji_pattern.sub(r'', tweet))\n",
        "            l.append(tweet.encode('ascii', 'ignore').decode('ascii'))\n",
        "        return l\n",
        "    \n",
        "        \n",
        "    def getCustomDataFramebylimits(df,lowerlimit):\n",
        "        main_df = df\n",
        "        #if(ifUpperLimit):\n",
        "        #    main_df = main_df[main_df['UserName'].map(main_df['UserName'].value_counts()) < upperlimit]\n",
        "        main_df = main_df[main_df['UserName'].map(main_df['UserName'].value_counts()) > lowerlimit]\n",
        "        Corp_df = pd.DataFrame({'author' : main_df['UserName'].unique()})\n",
        "        Corp_df['alltweets'] = [list(set(main_df['Tweet'].loc[main_df['UserName'] == x['author']])) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['Allhashtags'] = [getAllhashtags(x['alltweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['Uniquehashtags'] = [getuniquehashtags(x['alltweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['UniqueMentionedUsers'] = [getUniqueMentionedUsers(x['alltweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['AllMentionedUsers'] = [getAllMentionedUsers(x['alltweets']) for _,x in Corp_df.iterrows()]  \n",
        "        Corp_df['selectedtweets'] = [DeEmojify(x['alltweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['selectedtweets'] = [RemoveHTTP(x['selectedtweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['selectedtweets'] = [RemoveHttps(x['selectedtweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['selectedtweets'] = [RemoveWWW(x['selectedtweets']) for _,x in Corp_df.iterrows()]\n",
        "        Corp_df['selectedtweets'] = [CompletePreprocessingofWord(x['selectedtweets']) for _,x in Corp_df.iterrows()]\n",
        "        #Corp_df['selectedtweets'] = [RemoveTags(x['selectedtweets']) for _,x in Corp_df.iterrows()]   \n",
        "        #Corp_df['1000StackUserEmbb'] = []\n",
        "        return Corp_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTgggY2IHbCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetPosDocSimilarityMeanfromSimilarEmb(UserEmb):\n",
        "    SimilarUserCosineList = []\n",
        "    if(len(UserEmb) == 1):\n",
        "        return 1.0\n",
        "    for i in range(len(UserEmb)):\n",
        "        for j in range(i+1,len(UserEmb)):\n",
        "            res = 1 - spatial.distance.cosine(UserEmb[i],UserEmb[j])\n",
        "            SimilarUserCosineList.append(abs(res))\n",
        "    mean = sum(SimilarUserCosineList) / len(SimilarUserCosineList)\n",
        "    return mean\n",
        "def GetPosDocSimilarityMeanfromDifferentEmb(UserEmb1,UserEmb2):\n",
        "    DifferentUserCosineList = []\n",
        "    for i in range(len(UserEmb1)):\n",
        "        for j in range(len(UserEmb2)):\n",
        "            res = 1 - spatial.distance.cosine(UserEmb1[i],UserEmb2[j])\n",
        "            DifferentUserCosineList.append(abs(res))\n",
        "    mean = sum(DifferentUserCosineList) / len(DifferentUserCosineList)\n",
        "    return mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzGNIQSJgeyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Name_list = ['adriande','akshay','BBC','billgates','britneyspears','bruno','cnn','Cr7','drake','espn','fcbarca','harrystyles','iamSrk','instagram','Nasa','nytimes','obama','realMad','salman','sportscenter','srbachan','trump','twitter','virat','youtube']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGLvA63Rke68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from nltk import word_tokenize, pos_tag\n",
        "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
        "def GenerateImportantPOStagging(Alltweetlist):\n",
        "    keywords = []\n",
        "    for tweet in Alltweetlist:\n",
        "        token_lis = word_tokenize(tweet)\n",
        "        tagged_lis = pos_tag(token_lis)\n",
        "        keyword_l = [word for word, tag in tagged_lis if ((tag=='NNP') or (tag == 'NNPS') or (tag == 'NNS') or (tag == 'NN'))]\n",
        "        #print(tagged_lis)\n",
        "        for word in keyword_l[:]:\n",
        "            if word.startswith('http'):\n",
        "                keyword_l.remove(word)\n",
        "        keywords.extend(keyword_l)\n",
        "    #X=cv.fit_transform(keywords)\n",
        "    #k = list(cv.vocabulary_.keys())\n",
        "    return (' '.join(set(keywords)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhcNgrsFgtx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############  Training data for AutoEncoder and gensim models #########################################\n",
        "Data_F = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/training.1600000.processed.noemoticon.csv\",encoding = \"latin-1\")\n",
        "Data_F = Data_F[Data_F.columns[4:6]]\n",
        "Data_F.columns = [\"UserName\",\"Tweet\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we35-1QzkpOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = getCustomDataFramebylimits(Data_F,20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lMXh10KoVBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuvJFn4wk2zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df['SignificantPOS_tweets'] = [GenerateImportantPOStagging(x['selectedtweets']) for _,x in data_df.iterrows()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTHXXVYptwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df['SignificantPOS_tweets'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aZ_HvxnlrCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for i in range(4000):\n",
        "    corpus.append(data_df['SignificantPOS_tweets'][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26ny-WQlrFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "tagged_data = [TaggedDocument(words = word_tokenize(_d.lower()), tags = [str(i)]) for i,_d in enumerate(corpus)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeKK6AZImMCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epoch = 100\n",
        "vec_size = 2000\n",
        "alpha = 0.025\n",
        "model = Doc2Vec(vector_size = vec_size, alpha = alpha,min_alpha = 0.00025,min_count = 1,dm = 0)\n",
        "model.build_vocab(tagged_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx7qNJP6mMLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(max_epoch):\n",
        "    print('iteration{0}'.format(epoch))\n",
        "    model.train(tagged_data, total_examples = model.corpus_count, epochs = model.epochs)\n",
        "    model.alpha -= 0.0002\n",
        "    model.min_alpha = model.alpha\n",
        "    \n",
        "    if((epoch % 20) == 0):\n",
        "        model.save(\"NewDoc2Vec_\"+str(epoch)+\"T.model\")\n",
        "        print(\"Model Saved\")\n",
        "model.save(\"Final_Doc2Vec_POSincorporate.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8r7Q3supa8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### Training and testing Set for AutoEncoder#############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR7b2H0RpbIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POSDOC2Vecmodel = Doc2Vec.load(\"/content/NewDoc2Vec_100T.model\")\n",
        "final_Matrix_ds = []\n",
        "d = word_tokenize(data_df['SignificantPOS_tweets'][0].lower())\n",
        "vec = POSDOC2Vecmodel.infer_vector(d)\n",
        "final_Matrix_ds = np.concatenate((final_Matrix_ds,vec), axis = None)\n",
        "for _,x in data_df.iterrows():\n",
        "  data = word_tokenize(x['SignificantPOS_tweets'].lower())\n",
        "  vector = POSDOC2Vecmodel.infer_vector(data)\n",
        "  final_Matrix_ds = np.vstack((final_Matrix_ds,vector))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RLQtSiWnIsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,Flatten,Input\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yai1Q2ptnI0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(final_Matrix_ds)\n",
        "training_data, test_data = final_Matrix_ds[:4500,:],final_Matrix_ds[4500:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iYV5QlwnI7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding_dims = 300\n",
        "input_dim = Input(shape = (2000,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(2000, activation='tanh')(encoded)\n",
        "autoencoder = Model(input_dim,decoded)\n",
        "\n",
        "encoder = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7vf4E1boGUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.fit(training_data, training_data,\n",
        "                epochs=100,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_data, test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NvRWiOnoGhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "decoder.save('decoder.model')\n",
        "autoencoder.save('autoencoder.h5')\n",
        "encoder.save('encoder.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPRcplmoysuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################Building embeddings from csv directly############\n",
        "POSDOC2Vecmodel = Doc2Vec.load(\"/content/NewDoc2Vec_60T.model\")\n",
        "Encoder_model = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/encoder_2000to300.h5')\n",
        "def ReadandBuildEmbedding(csvname):\n",
        "  initial_lis = []\n",
        "  Readfilepath =  \"/content/drive/My Drive/Colab Notebooks/tweets/\" + str(csvname) + \".csv\"\n",
        "  Data_F = pd.read_csv(Readfilepath,encoding = \"latin-1\")\n",
        "  Data_F = Data_F[['Username','Tweet Content']]\n",
        "  Data_F.columns = [\"UserName\",\"Tweet\"]\n",
        "  data_df = getCustomDataFramebylimits(Data_F,20)\n",
        "  data_df['SignificantPOS_tweets'] = [GenerateImportantPOStagging(x['selectedtweets']) for _,x in data_df.iterrows()]\n",
        "  for _,x in data_df.iterrows():\n",
        "    fm_ds = []\n",
        "    d = word_tokenize(x['SignificantPOS_tweets'].lower())\n",
        "    vec = POSDOC2Vecmodel.infer_vector(d)\n",
        "    fm_ds = np.concatenate((fm_ds,vec), axis = None)\n",
        "    fm_ds = np.vstack((fm_ds,vec))\n",
        "    #print(fm_ds.shape)\n",
        "    UserEmbedding = Encoder_model.predict(fm_ds)\n",
        "    #print(UserEmbedding.shape)\n",
        "    initial_lis.append(UserEmbedding[0])\n",
        "\n",
        "  return initial_lis\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHKICfN23YYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_dic = {}\n",
        "for name in Name_list:\n",
        "  Main_dic[name] = ReadandBuildEmbedding(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75DK6movoFem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_dic['adriande']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dwxfYw79iQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labellist = []\n",
        "tokenlist = []\n",
        "colorlist = []\n",
        "#c = 0\n",
        "def preparedataforTsne(UserEmbedding,label,tokenlist):\n",
        "  c = len(set(labellist))\n",
        "  for x in UserEmbedding:\n",
        "    labellist.append(label)\n",
        "    tokenlist.append(x)\n",
        "    colorlist.append(colors[c])\n",
        "  #c = c + 1 \n",
        "  return tokenlist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab5GPDm0AHCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name in Name_list:\n",
        "    tokenlist = preparedataforTsne(Main_dic[name],name,tokenlist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fsCbRUWGISa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "x = np.arange(10)\n",
        "ys = [i+x+(i*x)**2 for i in range(25)]\n",
        "colors = cm.rainbow(np.linspace(0, 1, len(ys)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AaQJlbQGJ5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne_model = TSNE(perplexity = 50,n_components=2,init='pca',n_iter=2500, random_state = 23)\n",
        "new_values = tsne_model.fit_transform(tokenlist)\n",
        "x_cord = []\n",
        "y_cord = []\n",
        "\n",
        "for value in new_values:\n",
        "    x_cord.append(value[0])\n",
        "    y_cord.append(value[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAPrSyh3GS-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(25, 12)) \n",
        "for i in range(len(x_cord)):\n",
        "    plt.scatter(x_cord[i],y_cord[i], c = colorlist[i])\n",
        "    plt.annotate(labellist[i],xy=(x_cord[i], y_cord[i]),xytext=(5, 2),textcoords='offset points',ha='right',va='bottom')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jCawQ7GG9d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "User2DocSimilaritySameUser = {}  \n",
        "for name in Name_list:\n",
        "    User2DocSimilaritySameUser[name] = GetPosDocSimilarityMeanfromSimilarEmb(Main_dic[name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCEzwC80G9hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "User2DocSimilaritySameUser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmKY-Ioxsypr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_diff_dic = {}\n",
        "for main_name in Name_list:\n",
        "    differentUsersimilarity = {}\n",
        "    for name in Name_list:\n",
        "        if(name == main_name):\n",
        "            pass\n",
        "        else:\n",
        "            differentUsersimilarity[name] = GetPosDocSimilarityMeanfromDifferentEmb(Main_dic[main_name],Main_dic[name])\n",
        "    Main_diff_dic[main_name] = differentUsersimilarity\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVLltLLDsytr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_diff_dic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WUZSTzVsyxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX9sEI4SGd44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Difference_inSimilarityScore_PostDoc ={}\n",
        "for main_name in Name_list:\n",
        "    SimilarUserScore = User2DocSimilaritySameUser[main_name]\n",
        "    sum_of_difference = 0\n",
        "    dic = Main_diff_dic[main_name]\n",
        "    for name in Name_list:\n",
        "        if (name == main_name):\n",
        "            pass\n",
        "        else:  \n",
        "            s = abs(SimilarUserScore - dic[name])\n",
        "            sum_of_difference = sum_of_difference + s\n",
        "    Difference_inSimilarityScore_PostDoc[main_name] = sum_of_difference / 24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITjwxs9NuRhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Difference_inSimilarityScore_PostDoc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-3p6vxvuTWy",
        "colab_type": "code",
        "outputId": "377071f4-b42c-4577-cd52-0741b9a2ba8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s = 0\n",
        "for name in Name_list:\n",
        "  s = s + User2DocSimilaritySameUser[name]        #0.22, 0.67\n",
        "s / 25"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6775363818758464"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq5I2BbdukZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################## Auto Encoder for MultiView BioBert Combine and Graph Embedding ####################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-0UK-TK3Dve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TrainingMat = np.loadtxt('/content/drive/My Drive/Colab Notebooks/AutoEncoderInputMatrix/TrainingMatNew_Matrix.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qTGNOM3m3M8n",
        "outputId": "cdd15bb7-d451-488b-e443-d56cfbdbab66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,Flatten,Input\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJX4A2DO3Ege",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(TrainingMat)\n",
        "training_data, test_data = TrainingMat[:2500,:],TrainingMat[2500:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0gX0EHOq4cIE",
        "colab": {}
      },
      "source": [
        "encoding_dims = 300\n",
        "input_dim = Input(shape = (550,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(550, activation='tanh')(encoded)\n",
        "autoencoder = Model(input_dim,decoded)\n",
        "\n",
        "encoder = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TZOxFAbv4cIM",
        "colab": {}
      },
      "source": [
        "autoencoder.fit(training_data, training_data,\n",
        "                epochs=4000,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_data, test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk2B2LmT4pqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TestDataMatrix = np.loadtxt('/content/drive/My Drive/Colab Notebooks/AutoEncoderInputMatrix/TestingMatNew_Matrix.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z42xYeuO9tiI",
        "colab_type": "code",
        "outputId": "bbaa8a31-0dd4-4fc2-f28b-9596286c276b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(TestDataMatrix[0]).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(550,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofsofs7i71IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ResultMat = np.zeros(300)\n",
        "for i in range(TestDataMatrix.shape[0]):\n",
        "  vec = np.zeros(TestDataMatrix.shape[1])\n",
        "  vec = np.vstack((vec,TestDataMatrix[i]))\n",
        "  res = encoder.predict(vec)\n",
        "  ResultMat = np.vstack((ResultMat,res[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy9w9szU71La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ResultMat = ResultMat[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exN5k9KN-4Aw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = \"/content/drive/My Drive/Colab Notebooks/AutoEncoderInputMatrix/MultiViewBioBert_Graph_Emb300_Matrix.txt\"\n",
        "np.savetxt(fileName,ResultMat,fmt='%.8f') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jnkb4sm_TsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Qj-i8FYr8xc",
        "colab": {}
      },
      "source": [
        "################## Auto Encoder for MultiView BioBert Full and Graph Embedding and 1786 ####################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fUlinVer8xo",
        "colab": {}
      },
      "source": [
        "TrainingMat = np.loadtxt('/content/drive/My Drive/Colab Notebooks/AutoEncoderInputMatrix/TrainMatrix1786.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gfPh7Risr8xr",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,Flatten,Input\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJWk6T_jtJWh",
        "colab_type": "code",
        "outputId": "8ffba137-bb02-4c31-9508-f7c9d0df7488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TrainingMat.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3280, 1786)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qNgkCgT6r8xu",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(TrainingMat)\n",
        "training_data, test_data = TrainingMat[:2500,:],TrainingMat[2500:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lUxGGjMqr8xw",
        "colab": {}
      },
      "source": [
        "encoding_dims = 300\n",
        "input_dim = Input(shape = (1786,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(1786, activation='tanh')(encoded)\n",
        "autoencoder = Model(input_dim,decoded)\n",
        "\n",
        "encoder = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "raozUlBqr8x0",
        "colab": {}
      },
      "source": [
        "autoencoder.fit(training_data, training_data,\n",
        "                epochs=1000,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_data, test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pNcoXOQXuPXv",
        "colab": {}
      },
      "source": [
        "TestDataMatrix = np.loadtxt('/content/drive/My Drive/Colab Notebooks/AutoEncoderInputMatrix/SciBertTestMatrix1786.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "faAnEeeHuPX6",
        "colab": {}
      },
      "source": [
        "ResultMat = np.zeros(300)\n",
        "for i in range(TestDataMatrix.shape[0]):\n",
        "  vec = np.zeros(TestDataMatrix.shape[1])\n",
        "  vec = np.vstack((vec,TestDataMatrix[i]))\n",
        "  res = encoder.predict(vec)\n",
        "  ResultMat = np.vstack((ResultMat,res[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CR9CmQojuPX9",
        "colab": {}
      },
      "source": [
        "ResultMat = ResultMat[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Iyt-BFiuljL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ResultMat.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JF8VLmrkuPYB",
        "colab": {}
      },
      "source": [
        "fileName = \"/content/drive/My Drive/Colab Notebooks/AutoEncoderInputMatrix/Test_SciBERT_1786to300_Matrix.txt\"\n",
        "np.savetxt(fileName,ResultMat,fmt='%.8f') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anmVG5AKu7J9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}