{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The purpose of this notebook is to evaluate the embeddings on edge classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import re\n",
    "import networkx as nx\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Following Functions will extract important users mentioned or indicated to form nodes or vertices of Matrix\n",
    "\n",
    "#user name are mentioned in URL of the columns \"Twitter Reply to\",\"Twitter Retweet of\"\n",
    "def getUserFromUrl(Urlstr):\n",
    "    #print(Urlstr)\n",
    "    tok = Urlstr.split('/')\n",
    "    return tok[3]\n",
    "\n",
    "\n",
    "#Declaring some global variables\n",
    "All_User_for_Graph = set()   # Will consider all the users into account if mentioned tweeted retweeted for all the rows\n",
    "All_OccuranceList = []\n",
    "BigList = []\n",
    "#-----\n",
    "\n",
    "def GetTweetRetweetfromRow(row):\n",
    "    All_User_for_Graph.add(row['AUTHOR_ID'])\n",
    "    lis = []\n",
    "    if(row[\"Thread Entry Type\"] == 'post'):\n",
    "        return lis\n",
    "    else:\n",
    "        ReTweetList = row[\"Twitter Retweet of\"]\n",
    "        if(len(ReTweetList) >0):\n",
    "            for reTweet in ReTweetList:\n",
    "                if(pd.isnull(reTweet)):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = getUserFromUrl(reTweet)\n",
    "                    lis.append(name)\n",
    "                    All_User_for_Graph.add(name)\n",
    "        ReplyList = row['Twitter Reply to']\n",
    "        if(len(ReplyList) >0):\n",
    "            for reply in ReplyList:\n",
    "                if(pd.isnull(reply)):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = getUserFromUrl(reply)\n",
    "                    lis.append(name)\n",
    "                    All_User_for_Graph.add(name)\n",
    "        return list(set(lis))\n",
    "                \n",
    "def AllMentionedUser(row):\n",
    "    All_User_for_Graph.add(row['AUTHOR_ID'])\n",
    "    lis = []\n",
    "    tweetList = row[\"Snippet\"]\n",
    "    for tweet in tweetList:\n",
    "        lis.extend(re.findall(r\"@(\\w+)\",str(tweet)))\n",
    "    All_User_for_Graph.update(lis)\n",
    "    return list(set(lis))\n",
    "\n",
    "def AllConnections(row):\n",
    "    conn_lis = []\n",
    "    conn_lis.append(row['AUTHOR_ID'])\n",
    "    conn_lis.extend(row['TweetReTweetUser'])\n",
    "    conn_lis.extend(row['AllMentioneduser'])\n",
    "    return list(set(conn_lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTargetConnection(authorid):\n",
    "    totalList= []\n",
    "    for lis in BigList:\n",
    "        if(lis.count(authorid) > 0):\n",
    "            totalList.extend(lis)\n",
    "    k = list(filter((authorid).__ne__, totalList))\n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadMatfromPath(fileName):\n",
    "    ReadFilepath = \"C:/Users/SHAHBAZ/ZS UserEmbedding/BERT Embedding/\" + str(fileName) + \"_Matrix.txt\"\n",
    "    UserEmb = np.loadtxt(ReadFilepath)\n",
    "    return UserEmb\n",
    "def CombineMatrix(Mat1,Mat2):\n",
    "    Mat = np.zeros((Mat1.shape[1] + Mat2.shape[1]))\n",
    "    for i in range (1,Mat1.shape[0]):\n",
    "        vec = []\n",
    "        vec = np.concatenate((vec,Mat1[i]), axis=None)\n",
    "        vec = np.concatenate((vec,Mat2[i]), axis=None)\n",
    "        Mat = np.vstack((Mat,vec))\n",
    "    return Mat   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function important for Deep Walk###\n",
    "def get_randomwalk(node,path_length):\n",
    "    random_walk = [node]\n",
    "    for i in range(path_length-1):\n",
    "        temp = list(G.neighbors(node))\n",
    "        temp = list(set(temp) - set(random_walk))    \n",
    "        if len(temp) == 0:\n",
    "            break\n",
    "\n",
    "        random_node = random.choice(temp)\n",
    "        random_walk.append(random_node)\n",
    "        node = random_node\n",
    "        \n",
    "    return random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions important for Node2Vec ####\n",
    "def node2vec_walk(walk_length, start_node):\n",
    "    walk = [start_node]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        cur_nbrs = sorted(G.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            if len(walk) == 1:\n",
    "                walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "            else:\n",
    "                prev = walk[-2]\n",
    "                next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],alias_edges[(prev, cur)][1])]\n",
    "                walk.append(next)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return walk\n",
    "\n",
    "def simulate_walks(num_walks, walk_length):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    print ('Walk iteration:')\n",
    "    for walk_iter in range(num_walks):\n",
    "        print (str(walk_iter+1), '/', str(num_walks))\n",
    "        random.Random(7).shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "    return walks\n",
    "\n",
    "#give probabilty to edges on basis of p and q ( p is for dfs and q is for bfs)\n",
    "def get_alias_edge(src, dst):\n",
    "    unnormalized_probs = []\n",
    "    for dst_nbr in sorted(G.neighbors(dst)):\n",
    "        if dst_nbr == src:\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "        elif G.has_edge(dst_nbr, src):\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "        else:\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "    return alias_setup(normalized_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "    return J, q\n",
    "#set values to nodes and edges\n",
    "def preprocess_transition_probs():\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "        alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "    for edge in G.edges():\n",
    "        alias_edges[edge] = get_alias_edge(edge[0], edge[1])\n",
    "        alias_edges[(edge[1], edge[0])] = get_alias_edge(edge[1], edge[0])\n",
    "        \n",
    "def alias_draw(J, q):\n",
    "    K = len(J)\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Creating Training Data for Edge Classification #################\n",
    "#Useful Methods\n",
    "def non_neighbors_Custom(graph, node):\n",
    "    nbors = list(set(graph.neighbors(node)))   #| set([node])\n",
    "    lis = []\n",
    "    for nnode in All_user_Node:\n",
    "        if(nbors.count(nnode) == 0):\n",
    "            lis.append(nnode)\n",
    "    return lis\n",
    "\n",
    "def getAllUserNodeNotNeighbours(Graph,node,numbers):\n",
    "    non_neighbour = non_neighbors_Custom(Graph,node)\n",
    "    random.Random(7).shuffle(non_neighbour)\n",
    "    return non_neighbour[:numbers]\n",
    "\n",
    "def GetJaccardCoef(G,usr1,usr2):\n",
    "    cnbors = list(nx.common_neighbors(G, usr1, usr2))\n",
    "    union_size = len(set(G[usr1]) | set(G[usr2]))\n",
    "    score = len(cnbors) / union_size\n",
    "    return score\n",
    "\n",
    "def GetRessorceAllocationScore(G,usr1,usr2):\n",
    "    score = sum(1 / G.degree(w) for w in nx.common_neighbors(G, usr1, usr2))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSimilarityScore(nodeA,nodeB,Matrix):\n",
    "    rowd_usr1 = New_DF.loc[New_DF['AUTHOR_ID'] == nodeA]\n",
    "    usr1_ind = rowd_usr1.index.values.astype(int)[0]\n",
    "    rowd_usr2 = New_DF.loc[New_DF['AUTHOR_ID'] == nodeB]\n",
    "    #print(nodeB)\n",
    "    usr2_ind = rowd_usr2.index.values.astype(int)[0]\n",
    "        \n",
    "    usr1_vec = Matrix[usr1_ind]\n",
    "    usr2_vec = Matrix[usr2_ind]\n",
    "    res = 1 - spatial.distance.cosine(usr1_vec,usr2_vec)\n",
    "    \n",
    "    return abs(res)\n",
    "\n",
    "def isUserTarget(cnnectionList):\n",
    "    l = []\n",
    "    for c in cnnectionList:\n",
    "        if((All_user_Node.count(c)) > 0):\n",
    "            l.append(c)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateClassonCutoff(x1,x2,x3,cutoff):\n",
    "    z =  classifier.intercept_[0] + classifier.coef_[0][0] * x1 + classifier.coef_[0][1] * x2 + classifier.coef_[0][2] * x3\n",
    "    #z =  classifier.coef_[0][0] * x1 + classifier.coef_[0][1] * x2 + classifier.coef_[0][2] * x3\n",
    "    pred_prob = 1/ (1 + np.exp(-z))\n",
    "    if(pred_prob >= cutoff):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAccuracy_CutOff(xtest,ytest,cutoff):\n",
    "    ypred = []\n",
    "    for x in xtest:\n",
    "        ypred.append(CalculateClassonCutoff(x[0],x[1],x[2],cutoff))\n",
    "    cm = confusion_matrix(ytest, ypred)  \n",
    "    \n",
    "    precision = cm[0][0] / sum(cm[0])\n",
    "    recall = cm[0][0] / (cm[0][0] + cm[1][0])\n",
    "    accuracy = (cm[0][0] + cm[1][1]) / len(ypred)\n",
    "    print(\" Cut Off = \" + str(cutoff) + \"  Precision = \" + str(precision) + \" Recall = \" + str(recall) + \" Acurracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\SHAHBAZ\\Desktop\\study mat\\research papers\\New folder\\New folder\\20200507_Cardiology_HCP_Data.xlsx')\n",
    "Embedding_Column_Name = [\"AUTHOR_ID\", \"AUTHOR_NAME\",\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Location Name\",\"Gender\",\"Mentioned Authors\",\"Tags\",\"Twitter Reply to\",\"Twitter Retweet of\",\"Thread Entry Type\",\"Snippet\"]\n",
    "Main_DF = df[Embedding_Column_Name]\n",
    "Main_DF = Main_DF.groupby('AUTHOR_ID',as_index= False)[\"AUTHOR_NAME\",\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Location Name\",\"Gender\",\"Mentioned Authors\",\"Tags\",\"Twitter Reply to\",\"Twitter Retweet of\",\"Thread Entry Type\",\"Snippet\"].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Making a cleaner dataframe ####\n",
    "for columName in Embedding_Column_Name[1:]:\n",
    "    for _,x in Main_DF.iterrows():\n",
    "        x[columName] = list(set(x[columName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting additional user information from the data:\n",
    "Main_DF['TweetReTweetUser'] = [GetTweetRetweetfromRow(x) for _,x in Main_DF.iterrows()]\n",
    "Main_DF['AllMentioneduser'] = [AllMentionedUser(x) for _,x in Main_DF.iterrows()]  \n",
    "Main_DF['AllConnection'] = [AllConnections(x) for _,x in Main_DF.iterrows()] \n",
    "for _,x in Main_DF.iterrows():\n",
    "    All_OccuranceList.extend(x['AllConnection'])\n",
    "    BigList.append(x['AllConnection'])\n",
    "All_User = list(All_User_for_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_DF = Main_DF.head(4280)\n",
    "Main_DF['Target_Connection'] = [CreateTargetConnection(x['AUTHOR_ID']) for _,x in Main_DF.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_user_Node = []\n",
    "for _,x in Main_DF.iterrows():\n",
    "    All_user_Node.append(x['AUTHOR_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_DF = Main_DF[[\"AUTHOR_ID\",\"AllConnection\",\"Target_Connection\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating graph and adding nodes###\n",
    "G = nx.Graph()\n",
    "for user in All_User:\n",
    "    G.add_node(user)\n",
    "    \n",
    "### Connecting the edges for users who are linked ####\n",
    "for _,x in New_DF.iterrows():\n",
    "    source = x['AUTHOR_ID']\n",
    "    lis = x['Target_Connection']\n",
    "    \n",
    "    for target in lis:\n",
    "        G.add_edge(source, target)\n",
    "        G[source][target]['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implementing random walk ####\n",
    "all_nodes = list(G.nodes())\n",
    "\n",
    "random_walks = []\n",
    "for n in tqdm(all_nodes):\n",
    "    for i in range(10):\n",
    "        random_walks.append(get_randomwalk(n,10))\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train skip-gram (word2vec) model\n",
    "model = Word2Vec(window = 4, sg = 1, size = 100, hs = 0,negative = 10,alpha=0.03, min_alpha=0.0007,seed = 14)\n",
    "#size of 100,200 and 300 dimensions were tested and out of which 100 dimension performed best in MultiView\n",
    "model.build_vocab(random_walks, progress_per=2)\n",
    "model.train(random_walks, total_examples = model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepWalkMat = np.zeros(100)\n",
    "for _,x in New_DF.iterrows():\n",
    "    DeepWalkMat = np.vstack((DeepWalkMat,model[x['AUTHOR_ID']]))\n",
    "DeepWalkMat = DeepWalkMat[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Node2Vec #########\n",
    "p = 1\n",
    "q = 1 # p,q are the probabilties\n",
    "alias_nodes = {}\n",
    "alias_edges = {}\n",
    "preprocess_transition_probs()\n",
    "walks = simulate_walks(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip gram model to generate embedding from walks\n",
    "model = Word2Vec(window = 10, sg = 1, size = 100, hs = 0,negative = 10,alpha=0.03, min_alpha=0.0007,seed = 14)\n",
    "#size of 100,200 and 300 dimensions were tested and out of which 100 dimension performed best in MultiView\n",
    "model.build_vocab(walks, progress_per=2)\n",
    "model.train(walks, total_examples = model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node2VecMat = np.zeros(100)\n",
    "for _,x in New_DF.iterrows():\n",
    "    Node2VecMat = np.vstack((Node2VecMat,model[x['AUTHOR_ID']]))\n",
    "Node2VecMat = Node2VecMat[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Only considering nodes that are user and not a common topic of discussion\n",
    "### This is necessary to fetch corresponding user embeddings\n",
    "New_DF[\"UserTarget\"] = [isUserTarget(x['Target_Connection']) for _,x in New_DF.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Creating the DataFrame for Edge Classification without incorporating similarity score ######\n",
    "count = 1\n",
    "connections =[]\n",
    "\n",
    "###defining ####\n",
    "NodeA = []\n",
    "NodeB = []\n",
    "IsEdgeExist = []\n",
    "Jaccard_Coeeficient_list = []\n",
    "ResourceAllocationCoefflist = []\n",
    "\n",
    "for _,x in New_DF.iterrows():\n",
    "    node = x['AUTHOR_ID']\n",
    "    conn = x['UserTarget']\n",
    "    connections.clear()\n",
    "    for c in conn:\n",
    "        if((All_user_Node.count(c)) > 0):\n",
    "            connections.append(c)\n",
    "    non_lis = getAllUserNodeNotNeighbours(G,node,len(connections))\n",
    "    for nnode in non_lis:\n",
    "        NodeA.append(node)\n",
    "        NodeB.append(nnode)\n",
    "        Jaccard_Coeeficient_list.append(GetJaccardCoef(G,node,nnode))\n",
    "        ResourceAllocationCoefflist.append(GetRessorceAllocationScore(G,node,nnode))\n",
    "        IsEdgeExist.append(0)\n",
    "    \n",
    "    for conn in connections:\n",
    "        NodeA.append(node)\n",
    "        NodeB.append(conn)\n",
    "        Jaccard_Coeeficient_list.append(GetJaccardCoef(G,node,conn))\n",
    "        ResourceAllocationCoefflist.append(GetRessorceAllocationScore(G,node,conn))\n",
    "        IsEdgeExist.append(1)\n",
    "    \n",
    "    if(count % 500 == 0):\n",
    "        print(\"User \" + str(count) + \" completed out of \" + str(len(All_user_Node)))\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'main node': NodeA, 'neighbor node': NodeB, 'Resource Allocation Score': ResourceAllocationCoefflist, 'Jaccard coefficient score': Jaccard_Coeeficient_list,'Is_Edge_Exist':IsEdgeExist}\n",
    "df = pd.DataFrame(data, columns = ['main node','neighbor node','Resource Allocation Score','Jaccard coefficient score','Is_Edge_Exist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Post Doc Basic Evaluation ####\n",
    "### Read PostDoc matrix from path####\n",
    "postDoc_600 = ReadMatfromPath('PostDoc_Basic')\n",
    "postDoc_600 = postDoc_600[:4280,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train test split data after measuring the similarity score from the embedding matrix###\n",
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],postDoc_600) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the classifier to obtain coefficients corresponding to the predictor variables\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### validating the data to obtain decision boundary\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the classifier on test data####\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values\n",
    "CalculateAccuracy_CutOff(X_test,Y_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Clinical BERT####\n",
    "ClinicalBertSnippetEmbMat = ReadMatfromPath('ClinalBERT_SnippetEmbedding')\n",
    "ClinicalBertAuthorInfoEmbMat = ReadMatfromPath('ClinalBERT_Autho_Info_Embedding')\n",
    "ClinicalBertCombinedEmbMat = ReadMatfromPath('ClinalBERT_Combined_AutoEnc_Emb')\n",
    "\n",
    "ClinicalBertCombinedEmbMat = ClinicalBertCombinedEmbMat[1:,:]\n",
    "ClinicalBertSnippetEmbMat = ClinicalBertSnippetEmbMat[1:,:]\n",
    "ClinicalBertAuthorInfoEmbMat = ClinicalBertAuthorInfoEmbMat[1:,:]\n",
    "\n",
    "ClinicalBERT_1536_Mat = CombineMatrix(ClinicalBertSnippetEmbMat,ClinicalBertAuthorInfoEmbMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],ClinicalBERT_1536_Mat) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)\n",
    "\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAccuracy_CutOff(X_test,Y_test,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],ClinicalBertCombinedEmbMat) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)\n",
    "\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAccuracy_CutOff(X_test,Y_test,0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Scibert ############\n",
    "SciBertSnippetEmbMat = ReadMatfromPath('SciBERT_SnippetEmbedding')\n",
    "SciBertAuthorInfoEmbMat = ReadMatfromPath('SciBERT_Autho_Info_Embedding')\n",
    "SciBertCombinedEmbMat = ReadMatfromPath('SciBERT_Combined_Embedding')\n",
    "\n",
    "SciBertSnippetEmbMat = SciBertSnippetEmbMat[1:,:]\n",
    "SciBertAuthorInfoEmbMat = SciBertAuthorInfoEmbMat [1:,:]\n",
    "SciBertCombinedEmbMat = SciBertCombinedEmbMat [1:,:]\n",
    "SciBert_combine_1536 = CombineMatrix(SciBertSnippetEmbMat,SciBertAuthorInfoEmbMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],SciBert_combine_1536) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)\n",
    "\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAccuracy_CutOff(X_test,Y_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],SciBertCombinedEmbMat) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)\n",
    "\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAccuracy_CutOff(X_test,Y_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bio BERT #####\n",
    "BioBertSnippetEmbMat = ReadMatfromPath('BioBERT_SnippetEmb')\n",
    "BioBertAuthorInfoEmbMat = ReadMatfromPath('BioBERT_Autho_Info_Embedding')\n",
    "BioBERT_1536_Mat = CombineMatrix(BioBertSnippetEmbMat,BioBertAuthorInfoEmbMat)\n",
    "BioBERT_1536_Mat = BioBERT_1536_Mat[1: , :]\n",
    "BioBert_reduced_300 = ReadMatfromPath('BioBERT_Combined_AutoEnc_Emb')\n",
    "BioBert_reduced_300 = BioBert_reduced_300[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],BioBERT_1536_Mat) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)\n",
    "\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAccuracy_CutOff(X_test,Y_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],BioBert_reduced_300) for _,x in df.iterrows()]\n",
    "train_df = df.head(20000)\n",
    "test_df = df.tail(8912)\n",
    "validation_df = test_df.head(1912)\n",
    "test_df = test_df.tail(7000)\n",
    "\n",
    "X = train_df.iloc[:, [2, 3,5]].values\n",
    "Y = train_df.iloc[:, 4].values\n",
    "\n",
    "X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "Y_test = test_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0) \n",
    "classifier.fit(X, Y) \n",
    "print(\"Coefficients = \")\n",
    "print(classifier.coef_)\n",
    "print(\"Intercept\") \n",
    "print(classifier.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.0 \n",
    "while(c <= 1):\n",
    "    CalculateAccuracy_CutOff(X_val,Y_val,c)\n",
    "    c = c + 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAccuracy_CutOff(X_test,Y_test,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting the matrix of embedding for multi view using Deep walk and Node to vec and combining with other embedding \n",
    "### from pretrained BERT models.\n",
    "ClinicalBERT_1636_DW_Mat = CombineMatrix(ClinicalBERT_1536_Mat,DeepWalkMat)\n",
    "ClinicalBERT_400_DW_Mat = CombineMatrix(ClinicalBertCombinedEmbMat,DeepWalkMat)\n",
    "SciBERT_1636_DW_Mat = CombineMatrix(SciBert_combine_1536,DeepWalkMat)\n",
    "SciBERT_400_DW_Mat = CombineMatrix(SciBertCombinedEmbMat,DeepWalkMat)\n",
    "BioBERT_1636_DW_Mat = CombineMatrix(BioBERT_1536_Mat,DeepWalkMat)\n",
    "BioBERT_400_DW_Mat = CombineMatrix(BioBert_reduced_300,DeepWalkMat)\n",
    "\n",
    "ClinicalBERT_1636_Node2Vec_Mat = CombineMatrix(ClinicalBERT_1536_Mat,Node2VecMat)\n",
    "ClinicalBERT_400_Node2Vec_Mat = CombineMatrix(ClinicalBertCombinedEmbMat,Node2VecMat)\n",
    "SciBERT_1636_Node2Vec_Mat = CombineMatrix(SciBert_combine_1536,Node2VecMat)\n",
    "SciBERT_400_Node2Vec_Mat = CombineMatrix(SciBertCombinedEmbMat,Node2VecMat)\n",
    "BioBERT_1636_Node2Vec_Mat = CombineMatrix(BioBERT_1536_Mat,Node2VecMat)\n",
    "BioBERT_400_Node2Vec_Mat = CombineMatrix(BioBert_reduced_300,Node2VecMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining a method that will evaluate all the predictions at a cutoff of 0.1 which was the trend.\n",
    "def DoEvaluation(Matrix,name):\n",
    "    df['SimilarityScore'] = [GetSimilarityScore(x['main node'],x['neighbor node'],Matrix) for _,x in df.iterrows()]\n",
    "    train_df = df.head(20000)\n",
    "    test_df = df.tail(8912)\n",
    "    validation_df = test_df.head(1912)\n",
    "    test_df = test_df.tail(7000)\n",
    "\n",
    "    X = train_df.iloc[:, [2, 3,5]].values\n",
    "    Y = train_df.iloc[:, 4].values\n",
    "\n",
    "    X_val = validation_df.iloc[:, [2, 3,5]].values\n",
    "    Y_val = validation_df.iloc[:, 4].values\n",
    "\n",
    "    X_test = test_df.iloc[:, [2, 3,5]].values\n",
    "    Y_test = test_df.iloc[:, 4].values\n",
    "    \n",
    "    print(\"################### \"+ str(name) +\" ###################################\")\n",
    "    \n",
    "    classifier = LogisticRegression(random_state = 0) \n",
    "    classifier.fit(X, Y) \n",
    "    print(\"Coefficients = \")\n",
    "    print(classifier.coef_)\n",
    "    print(\"Intercept\") \n",
    "    print(classifier.intercept_)\n",
    "    \n",
    "    CalculateAccuracy_CutOff(X_test,Y_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Deep Walk #########\n",
    "DoEvaluation(ClinicalBERT_1636_DW_Mat,'ClinicalBERT_1636_DW')\n",
    "DoEvaluation(ClinicalBERT_400_DW_Mat,'ClinicalBERT_400_DW')\n",
    "DoEvaluation(SciBERT_1636_DW_Mat,'SciBERT_1636_DW')\n",
    "DoEvaluation(SciBERT_400_DW_Mat,'SciBERT_400_DW')\n",
    "DoEvaluation(BioBERT_1636_DW_Mat,'BioBERT_1636_DW')\n",
    "DoEvaluation(BioBERT_400_DW_Mat,'BioBERT_400_DW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Node2Vec #############\n",
    "DoEvaluation(ClinicalBERT_1636_Node2Vec_Mat,'ClinicalBERT_1636_Node2Vec')\n",
    "DoEvaluation(ClinicalBERT_400_Node2Vec_Mat,'ClinicalBERT_400_Node2Vec')\n",
    "DoEvaluation(SciBERT_1636_Node2Vec_Mat,'SciBERT_1636_Node2Vec')\n",
    "DoEvaluation(SciBERT_400_Node2Vec_Mat,'SciBERT_400_Node2Vec')\n",
    "DoEvaluation(BioBERT_1636_Node2Vec_Mat,'BioBERT_1636_Node2Vec')\n",
    "DoEvaluation(BioBERT_400_Node2Vec_Mat,'BioBERT_400_Node2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
