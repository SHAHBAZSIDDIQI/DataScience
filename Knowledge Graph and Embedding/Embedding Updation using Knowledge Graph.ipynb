{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Update Embedding_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "naZMTJrvdP6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The purpose of the notebook is to define the code for updating the user embedding \n",
        "#either using user doc approach or using post doc approach"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVHBNA2uO31w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular')\n",
        "from scipy import spatial\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn0_Q8KngWrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ Cleaning the tweets #################\n",
        "# lowercase\n",
        "# remove http\n",
        "# remove punctuations\n",
        "# remove @\n",
        "# remove #\n",
        "# remove stopwords or lemmatize\n",
        "\n",
        "def RemoveHTTP(tweet):\n",
        "  clean_tweet = re.match('(.*?)https.*?\\s?(.*?)',str(tweet))\n",
        "  if(clean_tweet):\n",
        "    return clean_tweet.group(1)\n",
        "  else:\n",
        "    return tweet\n",
        "\n",
        "def removePunc(inputstr):\n",
        "  for x in inputstr.lower():\n",
        "    if x in punctuations:\n",
        "      inputstr = inputstr.replace(x, \"\")\n",
        "  return inputstr\n",
        "\n",
        "def Clean(tweet):\n",
        "  clean_tweet = re.sub('@[^\\s]+','',str(tweet))   \n",
        "  clean_tweet = re.sub('#','',str(clean_tweet))\n",
        "  clean_tweet = re.sub('RT','',str(clean_tweet))\n",
        "  clean_tweet = removePunc(clean_tweet)\n",
        "  return clean_tweet\n",
        "  #tweet = tweet.strip()\n",
        "\n",
        "def removeStopWordsandLemmatize(inputstr):\n",
        "  token = word_tokenize(inputstr)\n",
        "  result = [i for i in token if not i in stop_words]\n",
        "  result = [lemmatizer.lemmatize(i) for i in result]\n",
        "  return ' '.join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6PcTAx0lHId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CompleteCleaning(tweet):\n",
        "  clean_tweet = RemoveHTTP(tweet)\n",
        "  clean_tweet = Clean(clean_tweet)\n",
        "  clean_tweet = removeStopWordsandLemmatize(clean_tweet)\n",
        "  return clean_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i4MlKgW4kjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,Flatten,Input\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGCBYRmiNeno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We have a pre trained autoencoder model that reduces the input vector of size 1536 to 768 dimension. \n",
        "# We are using the encoder model of the same autoencoder here to reduce the dimension\n",
        "from keras.models import load_model\n",
        "encoder_1536to768 = tf.keras.models.load_model('/content/drive/My Drive/Colab Notebooks/tweets/encoder_1536to768.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbKlaTnjz60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using bio bert model to generate embedding\n",
        "!pip install biobert-embedding==0.1.2\n",
        "from biobert_embedding.embedding import BiobertEmbedding\n",
        "biobert_model = BiobertEmbedding()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NOnNO3865Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function accepts a list of tweets which are text, clean them and then generate embedding of those one at a time, \n",
        "#Following which it merges two embedding at a time using the encoder. This is necessary the process of user doc method.\n",
        "def generateSnippetEmbeding(tweetList):\n",
        "  tweetEmbed = []\n",
        "  for tweet in tweetList:\n",
        "    emb = biobert_model.sentence_vector(CompleteCleaning(tweet))\n",
        "    tweetEmbed.append(emb)\n",
        "  mainvec = tweetEmbed[0]\n",
        "  if(len(tweetList) > 1):  \n",
        "    for i in range(1,len(tweetEmbed)):\n",
        "      vec = np.concatenate((mainvec,tweetEmbed[i]), axis = None)\n",
        "      fmax = vec\n",
        "      fmax = np.vstack((fmax,vec))\n",
        "      emb = encoder_1536to768.predict(fmax)\n",
        "      mainvec = emb[0]\n",
        "  return np.asarray(mainvec) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQgxTsEm65I4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function generate a vector of 1536 dimension vector , from 2 vcetors of 768 dimension each. One vector of 768 dim is generated from tweet data\n",
        "# another 768 dim vector is generated from user info. \n",
        "def Generate_1536Embedding(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding(rowd['Snippet'])\n",
        "  transAuthEmb = np.asarray(biobert_model.sentence_vector(CompleteCleaning(rowd['TRANS_AUTHOR_BIO'])))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  return np.asarray(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmKsQ4itPGgO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function is used to change or rather replace the Author Info only. \n",
        "def ChangeAuthorEmbOnly(rowd,embed):\n",
        "  tweetEmb,authoremb = embed[:768],embed[768:]\n",
        "  transAuthEmb = np.asarray(biobert_model.sentence_vector(CompleteCleaning(rowd['NewIntro'])))\n",
        "  vec = []\n",
        "  vec = np.concatenate((tweetEmb,transAuthEmb),axis = None)\n",
        "  return np.asarray(vec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6FMFdzOQDl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the best performing Bio BERT pre trained model update the new tweets to check if user embeddings are updated. \n",
        "def UpdateTweet_UserDoc_BioBert(row,embed):\n",
        "  tweetEmb,authoremb = embed[:768],embed[768:]\n",
        "  embedding_newtweet = generateSnippetEmbeding(row['newtweets'])\n",
        "  vec = np.concatenate((tweetEmb,embedding_newtweet), axis = None)\n",
        "  fmax = vec\n",
        "  fmax = np.vstack((fmax,vec))\n",
        "  emb = encoder_1536to768.predict(fmax)\n",
        "  snipetEmb = np.asarray(emb[0])\n",
        "  return np.asarray(np.concatenate((snipetEmb,authoremb),axis = None))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R79GrooeO71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/20200507_Cardiology_HCP_Data.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrK9ff51fb2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Embedding_Column_Name = [\"AUTHOR_ID\", \"TRANS_AUTHOR_BIO\",\"Account Type\",\"Snippet\"]\n",
        "Main_DF = df[Embedding_Column_Name]\n",
        "Main_DF = Main_DF.groupby('AUTHOR_ID',as_index= False)[\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Snippet\"].agg(lambda x: list(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFyK91Ff9Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove duplicate data\n",
        "for columName in Embedding_Column_Name[1:]:\n",
        "    for _,x in Main_DF.iterrows():\n",
        "        x[columName] = list(set(x[columName]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ0IGWroT3zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_DF['NewIntro'] = \"\"\n",
        "Main_DF['newtweets'] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKvrEIMhRlPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rowd = Main_DF.loc[Main_DF['AUTHOR_ID'] == 'BoniBlondie']   # Testing a user "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV4_EkMBSd0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "originalEmb = Generate_1536Embedding(rowd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAt6l2ETREN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rowd['NewIntro'] = 'pilot firefighter'   # updating with a new author Info to test user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgMZSyBtUbnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "authorEmbNew = ChangeAuthorEmbOnly(rowd,originalEmb)  # Changing only the author info part of the vector."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLRH2fAGUu_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Updating the tweets for the user, treating this as new tweets posted by the user.\n",
        "rowd['newtweets'][373] = ['RT @skathire As a cardiology consultant in the hospital, hereâ€™s my nomination for the heroes of the pandemic: 1. Nurses in the ER 2. Nurses in the ICU 3. Nurses on the floor 4. Nurses in the dialysis clinic 5. Nurses in the...']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4RLn4Y2WASv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Updating the tweet vector of the embedding \n",
        "TweetEmbNew = UpdateTweet_UserDoc_BioBert(rowd,originalEmb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqh58JIrW4pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Generating a matrix, this matrix could be downladed on local machine and evaluation could be performed on jupyter notebook code.\n",
        "Mat = np.zeros(1536)\n",
        "Mat = np.vstack((Mat,originalEmb))\n",
        "Mat = np.vstack((Mat,authorEmbNew))\n",
        "Mat = np.vstack((Mat,TweetEmbNew))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owD_H3yNXN2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Mat = Mat[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTyDbFgAXRSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/BioBERT_UserDoc_update_testuser2_Matrix.txt'\n",
        "np.savetxt(fileName,Mat,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}