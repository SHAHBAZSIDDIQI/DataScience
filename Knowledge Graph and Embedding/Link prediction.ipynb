{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This Notebook is useful to evaluate the embedding on Link Prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import re\n",
    "import networkx as nx\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Following Functions will extract important users mentioned or indicated to form nodes or vertices of Matrix\n",
    "#user name are mentioned in URL of the columns \"Twitter Reply to\",\"Twitter Retweet of\"\n",
    "def getUserFromUrl(Urlstr):\n",
    "    #print(Urlstr)\n",
    "    tok = Urlstr.split('/')\n",
    "    return tok[3]\n",
    "\n",
    "\n",
    "#Declaring some global variables\n",
    "All_User_for_Graph = set()   # Will consider all the users into account if mentioned tweeted retweeted for all the rows\n",
    "All_OccuranceList = []\n",
    "BigList = []\n",
    "#-----\n",
    "\n",
    "def GetTweetRetweetfromRow(row):\n",
    "    All_User_for_Graph.add(row['AUTHOR_ID'])\n",
    "    lis = []\n",
    "    if(row[\"Thread Entry Type\"] == 'post'):\n",
    "        return lis\n",
    "    else:\n",
    "        ReTweetList = row[\"Twitter Retweet of\"]\n",
    "        if(len(ReTweetList) >0):\n",
    "            for reTweet in ReTweetList:\n",
    "                if(pd.isnull(reTweet)):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = getUserFromUrl(reTweet)\n",
    "                    lis.append(name)\n",
    "                    All_User_for_Graph.add(name)\n",
    "        ReplyList = row['Twitter Reply to']\n",
    "        if(len(ReplyList) >0):\n",
    "            for reply in ReplyList:\n",
    "                if(pd.isnull(reply)):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = getUserFromUrl(reply)\n",
    "                    lis.append(name)\n",
    "                    All_User_for_Graph.add(name)\n",
    "        return list(set(lis))\n",
    "                \n",
    "def AllMentionedUser(row):\n",
    "    All_User_for_Graph.add(row['AUTHOR_ID'])\n",
    "    lis = []\n",
    "    tweetList = row[\"Snippet\"]\n",
    "    for tweet in tweetList:\n",
    "        lis.extend(re.findall(r\"@(\\w+)\",str(tweet)))\n",
    "    All_User_for_Graph.update(lis)\n",
    "    return list(set(lis))\n",
    "\n",
    "def AllConnections(row):\n",
    "    conn_lis = []\n",
    "    conn_lis.append(row['AUTHOR_ID'])\n",
    "    conn_lis.extend(row['TweetReTweetUser'])\n",
    "    conn_lis.extend(row['AllMentioneduser'])\n",
    "    return list(set(conn_lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\SHAHBAZ\\Desktop\\study mat\\research papers\\New folder\\New folder\\20200507_Cardiology_HCP_Data.xlsx')\n",
    "Embedding_Column_Name = [\"AUTHOR_ID\", \"AUTHOR_NAME\",\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Location Name\",\"Gender\",\"Mentioned Authors\",\"Tags\",\"Twitter Reply to\",\"Twitter Retweet of\",\"Thread Entry Type\",\"Snippet\"]\n",
    "Main_DF = df[Embedding_Column_Name]\n",
    "Main_DF = Main_DF.groupby('AUTHOR_ID',as_index= False)[\"AUTHOR_NAME\",\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Location Name\",\"Gender\",\"Mentioned Authors\",\"Tags\",\"Twitter Reply to\",\"Twitter Retweet of\",\"Thread Entry Type\",\"Snippet\"].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Making a cleaner dataframe ####\n",
    "for columName in Embedding_Column_Name[1:]:\n",
    "    for _,x in Main_DF.iterrows():\n",
    "        x[columName] = list(set(x[columName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting additional user information from the data:\n",
    "Main_DF['TweetReTweetUser'] = [GetTweetRetweetfromRow(x) for _,x in Main_DF.iterrows()]\n",
    "Main_DF['AllMentioneduser'] = [AllMentionedUser(x) for _,x in Main_DF.iterrows()]  \n",
    "Main_DF['AllConnection'] = [AllConnections(x) for _,x in Main_DF.iterrows()] \n",
    "for _,x in Main_DF.iterrows():\n",
    "    All_OccuranceList.extend(x['AllConnection'])\n",
    "    BigList.append(x['AllConnection'])\n",
    "All_User = list(All_User_for_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning the All Connection column to only accept users once and who is a connection only\n",
    "def CreateTargetConnection(authorid):\n",
    "    totalList= []\n",
    "    for lis in BigList:\n",
    "        if(lis.count(authorid) > 0):\n",
    "            totalList.extend(lis)\n",
    "    k = list(filter((authorid).__ne__, totalList))\n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_DF['Target_Connection'] = [CreateTargetConnection(x['AUTHOR_ID']) for _,x in Main_DF.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_DF = Main_DF.head(4280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The list that keeps record of all the users whose embeddings are available, so we can calculate similarity score for them\n",
    "All_user_Node = []\n",
    "for _,x in Main_DF.iterrows():\n",
    "    All_user_Node.append(x['AUTHOR_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the test data. Important to Note taking into account only users who have atleast 3 other users in their neighbor\n",
    "## Also sub dividing to mini list of users and their neighbours in order to avoid huge computation of all the user nodes.\n",
    "Test_EdgeList = []\n",
    "Test_MainUsers = []\n",
    "Test_NeighborUsers = []\n",
    "def GeneratetestedgeList(node,connectionlist):\n",
    "    connection_list_user = []\n",
    "    connection_copy = connectionlist.copy()\n",
    "    for users in connection_copy:\n",
    "        if(All_user_Node.count(users) > 0):\n",
    "            connection_list_user.append(users)\n",
    "    if(len(connection_list_user) >= 3):\n",
    "        n = math.ceil( 0.5 * len(connection_list_user))\n",
    "        #random.Random(3).shuffle(connection_list_user)\n",
    "        l = random.sample(connection_list_user,n)\n",
    "        Test_MainUsers.append(node)\n",
    "        for inode in l:\n",
    "            temp = [node]\n",
    "            temp.append(inode)\n",
    "            if(Test_EdgeList.count(temp) == 0):\n",
    "                Test_NeighborUsers.append(inode)\n",
    "                Test_EdgeList.append(temp)\n",
    "                connection_copy.remove(inode)\n",
    "    return connection_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_DF = Main_DF[[\"AUTHOR_ID\",\"AllConnection\",\"Target_Connection\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_DF[\"new_Target_Connection\"] = [GeneratetestedgeList(x['AUTHOR_ID'],x['Target_Connection']) for _,x in New_DF.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_NeighborUsers = list(set(Test_NeighborUsers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking if all the test data nodes have embeddings or not.\n",
    "present = 0\n",
    "notpresent  = 0\n",
    "for edge in Test_EdgeList:\n",
    "    a,b = edge[0],edge[1]\n",
    "    if((Test_MainUsers.count(a) > 0) and (Test_NeighborUsers.count(b) > 0)):\n",
    "        present = present + 1\n",
    "    else:\n",
    "        notpresent = notpresent + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_EdgeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadMatfromPath(fileName):\n",
    "    ReadFilepath = \"C:/Users/SHAHBAZ/ZS UserEmbedding/BERT Embedding/\" + str(fileName) + \"_Matrix.txt\"\n",
    "    UserEmb = np.loadtxt(ReadFilepath)\n",
    "    return UserEmb\n",
    "def CombineMatrix(Mat1,Mat2):\n",
    "    Mat = np.zeros((Mat1.shape[1] + Mat2.shape[1]))\n",
    "    for i in range (1,Mat1.shape[0]):\n",
    "        vec = []\n",
    "        vec = np.concatenate((vec,Mat1[i]), axis=None)\n",
    "        vec = np.concatenate((vec,Mat2[i]), axis=None)\n",
    "        Mat = np.vstack((Mat,vec))\n",
    "    return Mat   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### creating deep walk embedding #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating graph and adding nodes###\n",
    "G = nx.Graph()\n",
    "for user in All_User:\n",
    "    G.add_node(user)\n",
    "    \n",
    "### Connecting the edges for users who are linked ####\n",
    "for _,x in New_DF.iterrows():\n",
    "    source = x['AUTHOR_ID']\n",
    "    lis = x['new_Target_Connection']\n",
    "    \n",
    "    for target in lis:\n",
    "        G.add_edge(source, target)\n",
    "        G[source][target]['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randomwalk(node,path_length):\n",
    "    random_walk = [node]\n",
    "    for i in range(path_length-1):\n",
    "        temp = list(G.neighbors(node))\n",
    "        temp = list(set(temp) - set(random_walk))    \n",
    "        if len(temp) == 0:\n",
    "            break\n",
    "\n",
    "        random_node = random.choice(temp)\n",
    "        random_walk.append(random_node)\n",
    "        node = random_node\n",
    "        \n",
    "    return random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implementing random walk ####\n",
    "all_nodes = list(G.nodes())\n",
    "\n",
    "random_walks = []\n",
    "for n in tqdm(all_nodes):\n",
    "    for i in range(10):\n",
    "        random_walks.append(get_randomwalk(n,10))\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train skip-gram (word2vec) model\n",
    "model = Word2Vec(window = 4, sg = 1, size = 100, hs = 0,negative = 10,alpha=0.03, min_alpha=0.0007,seed = 14)\n",
    "#size of 100,200 and 300 dimensions were tested and out of which 100 dimension performed best in MultiView\n",
    "model.build_vocab(random_walks, progress_per=2)\n",
    "model.train(random_walks, total_examples = model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepWalkMat = np.zeros(100)\n",
    "for _,x in New_DF.iterrows():\n",
    "    DeepWalkMat = np.vstack((DeepWalkMat,model[x['AUTHOR_ID']]))\n",
    "DeepWalkMat = DeepWalkMat[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Creating Node2Vec Embedding #########\n",
    "p = 1\n",
    "q = 1 # p,q are the probabilties\n",
    "alias_nodes = {}\n",
    "alias_edges = {}\n",
    "#Main part that generates the walk\n",
    "def node2vec_walk(walk_length, start_node):\n",
    "    walk = [start_node]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        cur_nbrs = sorted(G.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            if len(walk) == 1:\n",
    "                walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "            else:\n",
    "                prev = walk[-2]\n",
    "                next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],alias_edges[(prev, cur)][1])]\n",
    "                walk.append(next)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return walk\n",
    "\n",
    "def simulate_walks(num_walks, walk_length):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    print ('Walk iteration:')\n",
    "    for walk_iter in range(num_walks):\n",
    "        print (str(walk_iter+1), '/', str(num_walks))\n",
    "        random.Random(7).shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "    return walks\n",
    "\n",
    "#give probabilty to edges on basis of p and q ( p is for dfs and q is for bfs)\n",
    "def get_alias_edge(src, dst):\n",
    "    unnormalized_probs = []\n",
    "    for dst_nbr in sorted(G.neighbors(dst)):\n",
    "        if dst_nbr == src:\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "        elif G.has_edge(dst_nbr, src):\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "        else:\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "    return alias_setup(normalized_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "    return J, q\n",
    "#set values to nodes and edges\n",
    "def preprocess_transition_probs():\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "        alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "    for edge in G.edges():\n",
    "        alias_edges[edge] = get_alias_edge(edge[0], edge[1])\n",
    "        alias_edges[(edge[1], edge[0])] = get_alias_edge(edge[1], edge[0])\n",
    "        \n",
    "def alias_draw(J, q):\n",
    "    K = len(J)\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transition_probs()\n",
    "walks = simulate_walks(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip gram model to generate embedding from walks\n",
    "model = Word2Vec(window = 10, sg = 1, size = 100, hs = 0,negative = 10,alpha=0.03, min_alpha=0.0007,seed = 14)\n",
    "#size of 100,200 and 300 dimensions were tested and out of which 100 dimension performed best in MultiView\n",
    "model.build_vocab(walks, progress_per=2)\n",
    "model.train(walks, total_examples = model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node2VecMat = np.zeros(100)\n",
    "for _,x in New_DF.iterrows():\n",
    "    Node2VecMat = np.vstack((Node2VecMat,model[x['AUTHOR_ID']]))\n",
    "Node2VecMat = Node2VecMat[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this function calculate the mean similarity score of a node with its neighbors\n",
    "def CalculateNeighbourMeanSimilarity(node,neighbourList,Matrix):\n",
    "    count = 0\n",
    "    simi_sum = 0\n",
    "    rowd = New_DF.loc[New_DF['AUTHOR_ID'] == node]\n",
    "    node_ind = rowd.index.values.astype(int)[0]\n",
    "    node_vec = Matrix[node_ind]\n",
    "    \n",
    "    for neigh_node in neighbourList:\n",
    "        rowd = New_DF.loc[New_DF['AUTHOR_ID'] == neigh_node]\n",
    "        if(len(rowd['AUTHOR_ID']) > 0):\n",
    "            neigh_ind = rowd.index.values.astype(int)[0]\n",
    "            neigh_vec = Matrix[neigh_ind]\n",
    "            res = 1 - spatial.distance.cosine(node_vec,neigh_vec)\n",
    "            simi_sum = simi_sum + abs(res)\n",
    "            count = count + 1\n",
    "    if(count > 0):\n",
    "        return simi_sum / count\n",
    "    else:\n",
    "        return 0.8       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_neighbors(graph, node):\n",
    "    nbors = list(set(graph.neighbors(node)))   #| set([node])\n",
    "    lis = []\n",
    "    for nnode in Test_NeighborUsers:\n",
    "        if(nbors.count(nnode) == 0):\n",
    "            lis.append(nnode)\n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the number of edges possible between unconnected nodes\n",
    "def non_edges(graph):\n",
    "    if graph.is_directed():\n",
    "        for u in Test_MainUsers:   #modified\n",
    "            for v in non_neighbors(graph, u):\n",
    "                #yield (u, v)\n",
    "                l = [u,v]\n",
    "                return l\n",
    "    else:\n",
    "        S = []\n",
    "        count = 1\n",
    "        for u in Test_MainUsers:\n",
    "            for v in non_neighbors(graph, u):\n",
    "                l = [u,v]\n",
    "                if(S.count(l) == 0):\n",
    "                    S.append(l)\n",
    "            if((count % 10) == 0):\n",
    "                print(\"Epoch = \" + str(count) + \" / \" + str(len(Test_MainUsers)))\n",
    "            count = count + 1\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebunch = non_edges(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ebunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### method to evaluate if the link between 2 nodes should be established using resource allocation score and then using\n",
    "## The cosine similarity score between the nodes and establishment is done only when the similarity score is above \n",
    "## a multiplicative factor of nodes neighbor hood mean score. The method takes embedding matrix and cutoff score as input.\n",
    "def predictlink_resourceAllocation(usr1,usr2,Matrix,cutoff_sc):\n",
    "    score = sum(1 / G.degree(w) for w in nx.common_neighbors(G, usr1, usr2))\n",
    "    if(score > 0):\n",
    "        rowd_usr1 = New_DF.loc[New_DF['AUTHOR_ID'] == usr1]\n",
    "        usr1_sim = rowd_usr1['Mean_Neighbour_Similarity']\n",
    "        usr1_ind = rowd_usr1.index.values.astype(int)[0]\n",
    "        rowd_usr2 = New_DF.loc[New_DF['AUTHOR_ID'] == usr2]\n",
    "        usr2_sim = rowd_usr2['Mean_Neighbour_Similarity']\n",
    "        usr2_ind = rowd_usr2.index.values.astype(int)[0]\n",
    "        \n",
    "        usr1_vec = Matrix[usr1_ind]\n",
    "        usr2_vec = Matrix[usr2_ind]\n",
    "        \n",
    "        res = 1 - spatial.distance.cosine(usr1_vec,usr2_vec)\n",
    "        if(abs(res) > (cutoff_sc * max(usr1_sim.values[0],usr2_sim.values[0]))):\n",
    "            temp1 = [usr1,usr2]\n",
    "            corr_pred_list_resource.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### method to evaluate if the link between 2 nodes should be established using Jaccard Coefficient score and then using\n",
    "## The cosine similarity score between the nodes and establishment is done only when the similarity score is above \n",
    "## a multiplicative factor of nodes neighbor hood mean score. The method takes embedding matrix and cutoff score as input.\n",
    "def predict_link_JaccardCoeff(usr1,usr2,Matrix,cutoff_sc):\n",
    "    cnbors = list(nx.common_neighbors(G, usr1, usr2))\n",
    "    union_size = len(set(G[usr1]) | set(G[usr2]))\n",
    "    score = len(cnbors) / union_size\n",
    "    if(score > 0):\n",
    "        rowd_usr1 = New_DF.loc[New_DF['AUTHOR_ID'] == usr1]\n",
    "        usr1_sim = rowd_usr1['Mean_Neighbour_Similarity']\n",
    "        usr1_ind = rowd_usr1.index.values.astype(int)[0]\n",
    "        rowd_usr2 = New_DF.loc[New_DF['AUTHOR_ID'] == usr2]\n",
    "        usr2_sim = rowd_usr2['Mean_Neighbour_Similarity']\n",
    "        usr2_ind = rowd_usr2.index.values.astype(int)[0]\n",
    "        \n",
    "        usr1_vec = Matrix[usr1_ind]\n",
    "        usr2_vec = Matrix[usr2_ind]\n",
    "        \n",
    "        res = 1 - spatial.distance.cosine(usr1_vec,usr2_vec)\n",
    "        if(abs(res) > (cutoff_sc * max(usr1_sim.values[0],usr2_sim.values[0]))):\n",
    "            temp1 = [usr1,usr2]\n",
    "            corr_pred_list_jacard.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompleteLinkPrediction_Resource(Matrix,cutoff_sc):    \n",
    "    #ebunch = non_edges(G) # calculating the non-edges\n",
    "    count_alo = 1\n",
    "    for t in ebunch:  # was list around ebunch\n",
    "        predictlink_resourceAllocation(t[0],t[1],Matrix,cutoff_sc)\n",
    "        if(count_alo % 50000 == 0):\n",
    "            print(\"Epoch Resource Allocation \" + str (count_alo) + \" / \" + str(len(ebunch)))\n",
    "        count_alo = count_alo + 1\n",
    "    corr_pred = 0\n",
    "    for test_edge in Test_EdgeList:\n",
    "        if(corr_pred_list_resource.count(test_edge) > 0):\n",
    "            corr_pred = corr_pred + 1\n",
    "    print(\"edges predicted =\" + str(len(corr_pred_list_resource)))\n",
    "    print(\"Precision \" + str(corr_pred / len(Test_EdgeList)))\n",
    "    print(\"Recall \" + str(corr_pred / len(corr_pred_list_resource)))\n",
    "    print(\"Resource allocation correctly predicted = \"+str(corr_pred) + \" / \" + str(len(Test_EdgeList)) + \" accuracy= \" + str(corr_pred/len(Test_EdgeList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompleteLinkPrediction_Jaccard(Matrix,cutoff_sc):   \n",
    "    #ebunch = non_edges(G) # calculating the non-edges\n",
    "    count = 1\n",
    "    for t in ebunch:\n",
    "        predict_link_JaccardCoeff(t[0],t[1],Matrix,cutoff_sc)\n",
    "        if(count % 50000 == 0):\n",
    "            print(\"Epoch Resource Jaccard \" + str (count) + \" / \" + str(len(ebunch)))\n",
    "        count = count + 1\n",
    "    corr_pred = 0\n",
    "    for test_edge in Test_EdgeList:\n",
    "        if(corr_pred_list_jacard.count(test_edge) > 0):\n",
    "            corr_pred = corr_pred + 1\n",
    "    print(\"edges predicted =\" + str(len(corr_pred_list_jacard)))\n",
    "    recall = corr_pred / len(Test_EdgeList)\n",
    "    prec = corr_pred / (len(corr_pred_list_jacard) - len(Test_EdgeList) + corr_pred) \n",
    "    print(\"Precision \" + str(prec))    \n",
    "    print(\"Recall \" + str(recall))\n",
    "    f1score = (2 * prec * recall) / (prec + recall)\n",
    "    print(\"F1 score \" + str(f1score))\n",
    "    print(\"Jaccard Coefficient correctly predicted = \"+str(corr_pred) + \" / \" + str(len(Test_EdgeList)) + \" accuracy= \" + str(corr_pred/len(Test_EdgeList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The method that use previous defined methods to print the result takes embeding matrix and cutoff score as input\n",
    "def CompleteEval(Matrix,cutoff_sc):\n",
    "    New_DF['Mean_Neighbour_Similarity'] = \"\"\n",
    "    for _,x in New_DF.iterrows():\n",
    "        x['Mean_Neighbour_Similarity'] = CalculateNeighbourMeanSimilarity(x['AUTHOR_ID'],x['new_Target_Connection'],Matrix)\n",
    "    print(\"Mean similarity for users calculated\")\n",
    "    corr_pred_list_resource = []\n",
    "    corr_pred_list_jacard = []\n",
    "    \n",
    "    CompleteLinkPrediction_Jaccard(Matrix,cutoff_sc)\n",
    "    #CompleteLinkPrediction_Resource(Matrix,cutoff_sc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postDoc_600 = ReadMatfromPath('PostDoc_Basic')\n",
    "postDoc_600 = postDoc_600[:4280,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(postDoc_600,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClinicalBertSnippetEmbMat = ReadMatfromPath('ClinalBERT_SnippetEmbedding')\n",
    "ClinicalBertAuthorInfoEmbMat = ReadMatfromPath('ClinalBERT_Autho_Info_Embedding')\n",
    "ClinicalBertCombinedEmbMat = ReadMatfromPath('ClinalBERT_Combined_AutoEnc_Emb')\n",
    "\n",
    "ClinicalBertCombinedEmbMat = ClinicalBertCombinedEmbMat[1:,:]\n",
    "ClinicalBertSnippetEmbMat = ClinicalBertSnippetEmbMat[1:,:]\n",
    "ClinicalBertAuthorInfoEmbMat = ClinicalBertAuthorInfoEmbMat[1:,:]\n",
    "\n",
    "ClinicalBERT_1536_Mat = CombineMatrix(ClinicalBertSnippetEmbMat,ClinicalBertAuthorInfoEmbMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(ClinicalBERT_1536_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(ClinicalBertCombinedEmbMat,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Scibert ############\n",
    "SciBertSnippetEmbMat = ReadMatfromPath('SciBERT_SnippetEmbedding')\n",
    "SciBertAuthorInfoEmbMat = ReadMatfromPath('SciBERT_Autho_Info_Embedding')\n",
    "SciBertCombinedEmbMat = ReadMatfromPath('SciBERT_Combined_Embedding')\n",
    "\n",
    "SciBertSnippetEmbMat = SciBertSnippetEmbMat[1:,:]\n",
    "SciBertAuthorInfoEmbMat = SciBertAuthorInfoEmbMat [1:,:]\n",
    "SciBertCombinedEmbMat = SciBertCombinedEmbMat [1:,:]\n",
    "SciBert_combine_1536 = CombineMatrix(SciBertSnippetEmbMat,SciBertAuthorInfoEmbMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(SciBert_combine_1536,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(SciBertCombinedEmbMat,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ creating data frame for a single node #####\n",
    "#ebunch  #ASabanayagamMD\n",
    "nodeA = []\n",
    "nodeB = []\n",
    "similarityScore = []\n",
    "Jaccard_coeeficient = []\n",
    "Actual_connection = []\n",
    "count = 1\n",
    "for lis in ebunch:\n",
    "    if (lis[0] == 'ASabanayagamMD'):\n",
    "        nodeA.append(lis[0])\n",
    "        nodeB.append(lis[1])\n",
    "        cnbors = list(nx.common_neighbors(G, lis[0], lis[1]))\n",
    "        union_size = len(set(G[lis[0]]) | set(G[lis[1]]))\n",
    "        score = len(cnbors) / union_size\n",
    "        Jaccard_coeeficient.append(score)\n",
    "        rowd_usr1 = New_DF.loc[New_DF['AUTHOR_ID'] == lis[0]]\n",
    "        usr1_ind = rowd_usr1.index.values.astype(int)[0]\n",
    "        rowd_usr2 = New_DF.loc[New_DF['AUTHOR_ID'] == lis[1]]\n",
    "        usr2_ind = rowd_usr2.index.values.astype(int)[0]\n",
    "        \n",
    "        usr1_vec = postDoc_600[usr1_ind]\n",
    "        usr2_vec = postDoc_600[usr2_ind]\n",
    "        \n",
    "        res = 1 - spatial.distance.cosine(usr1_vec,usr2_vec)\n",
    "        similarityScore.append(abs(res))\n",
    "        if(Test_EdgeList.count(lis) > 0):\n",
    "            Actual_connection.append(1)\n",
    "        else:\n",
    "            Actual_connection.append(0)\n",
    "    if(count % 50000 == 0):\n",
    "        print(\"Epoch \" + str(count) + \" / 266090\")\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'main node': nodeA, 'neighbor node': nodeB, 'cosine similarity': similarityScore, 'Jaccard coefficient score': Jaccard_coeeficient,'Actual_connection':Actual_connection}\n",
    "df = pd.DataFrame(data, columns = ['main node','neighbor node','cosine similarity','Jaccard coefficient score','Actual_connection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('SingleNodeScore.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############3end################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BioBertSnippetEmbMat = ReadMatfromPath('BioBERT_SnippetEmb')\n",
    "BioBertAuthorInfoEmbMat = ReadMatfromPath('BioBERT_Autho_Info_Embedding')\n",
    "BioBERT_1536_Mat = CombineMatrix(BioBertSnippetEmbMat,BioBertAuthorInfoEmbMat)\n",
    "BioBERT_1536_Mat = BioBERT_1536_Mat[1: , :]\n",
    "BioBert_reduced_300 = ReadMatfromPath('BioBERT_Combined_AutoEnc_Emb')\n",
    "BioBert_reduced_300 = BioBert_reduced_300[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(BioBERT_1536_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(BioBert_reduced_300,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClinicalBERT_1636_DW_Mat = CombineMatrix(ClinicalBERT_1536_Mat,DeepWalkMat)\n",
    "ClinicalBERT_400_DW_Mat = CombineMatrix(ClinicalBertCombinedEmbMat,DeepWalkMat)\n",
    "SciBERT_1636_DW_Mat = CombineMatrix(SciBert_combine_1536,DeepWalkMat)\n",
    "SciBERT_400_DW_Mat = CombineMatrix(SciBertCombinedEmbMat,DeepWalkMat)\n",
    "BioBERT_1636_DW_Mat = CombineMatrix(BioBERT_1536_Mat,DeepWalkMat)\n",
    "BioBERT_400_DW_Mat = CombineMatrix(BioBert_reduced_300,DeepWalkMat)\n",
    "\n",
    "ClinicalBERT_1636_Node2Vec_Mat = CombineMatrix(ClinicalBERT_1536_Mat,Node2VecMat)\n",
    "ClinicalBERT_400_Node2Vec_Mat = CombineMatrix(ClinicalBertCombinedEmbMat,Node2VecMat)\n",
    "SciBERT_1636_Node2Vec_Mat = CombineMatrix(SciBert_combine_1536,Node2VecMat)\n",
    "SciBERT_400_Node2Vec_Mat = CombineMatrix(SciBertCombinedEmbMat,Node2VecMat)\n",
    "BioBERT_1636_Node2Vec_Mat = CombineMatrix(BioBERT_1536_Mat,Node2VecMat)\n",
    "BioBERT_400_Node2Vec_Mat = CombineMatrix(BioBert_reduced_300,Node2VecMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(ClinicalBERT_1636_DW_Mat,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(ClinicalBERT_400_DW_Mat,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(SciBERT_1636_DW_Mat,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(SciBERT_400_DW_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(BioBERT_1636_DW_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(BioBERT_400_DW_Mat,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(ClinicalBERT_1636_Node2Vec_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(ClinicalBERT_400_Node2Vec_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(SciBERT_1636_Node2Vec_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(SciBERT_400_Node2Vec_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(BioBERT_1636_Node2Vec_Mat,0.8)\n",
    "\n",
    "corr_pred_list_resource = []\n",
    "corr_pred_list_jacard = []\n",
    "CompleteEval(BioBERT_400_Node2Vec_Mat,0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
