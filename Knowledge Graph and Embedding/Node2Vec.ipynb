{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The motive of this notebook is to generate user embedding using Node2Vec approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Following Functions will extract important users mentioned or indicated to form nodes or vertices of Matrix\n",
    "\n",
    "#user name are mentioned in URL of the columns \"Twitter Reply to\",\"Twitter Retweet of\"\n",
    "def getUserFromUrl(Urlstr):\n",
    "    #print(Urlstr)\n",
    "    tok = Urlstr.split('/')\n",
    "    return tok[3]\n",
    "\n",
    "\n",
    "#Declaring some global variables\n",
    "All_User_for_Graph = set()   # Will consider all the users into account if mentioned tweeted retweeted for all the rows\n",
    "All_OccuranceList = []\n",
    "BigList = []\n",
    "#-----\n",
    "\n",
    "def GetTweetRetweetfromRow(row):\n",
    "    All_User_for_Graph.add(row['AUTHOR_ID'])\n",
    "    lis = []\n",
    "    if(row[\"Thread Entry Type\"] == 'post'):\n",
    "        return lis\n",
    "    else:\n",
    "        ReTweetList = row[\"Twitter Retweet of\"]\n",
    "        if(len(ReTweetList) >0):\n",
    "            for reTweet in ReTweetList:\n",
    "                if(pd.isnull(reTweet)):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = getUserFromUrl(reTweet)\n",
    "                    lis.append(name)\n",
    "                    All_User_for_Graph.add(name)\n",
    "        ReplyList = row['Twitter Reply to']\n",
    "        if(len(ReplyList) >0):\n",
    "            for reply in ReplyList:\n",
    "                if(pd.isnull(reply)):\n",
    "                    pass\n",
    "                else:\n",
    "                    name = getUserFromUrl(reply)\n",
    "                    lis.append(name)\n",
    "                    All_User_for_Graph.add(name)\n",
    "        return list(set(lis))\n",
    "                \n",
    "def AllMentionedUser(row):\n",
    "    All_User_for_Graph.add(row['AUTHOR_ID'])\n",
    "    lis = []\n",
    "    tweetList = row[\"Snippet\"]\n",
    "    for tweet in tweetList:\n",
    "        lis.extend(re.findall(r\"@(\\w+)\",str(tweet)))\n",
    "    All_User_for_Graph.update(lis)\n",
    "    return list(set(lis))\n",
    "\n",
    "def AllConnections(row):\n",
    "    conn_lis = []\n",
    "    conn_lis.append(row['AUTHOR_ID'])\n",
    "    conn_lis.extend(row['TweetReTweetUser'])\n",
    "    conn_lis.extend(row['AllMentioneduser'])\n",
    "    return list(set(conn_lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\SHAHBAZ\\Desktop\\study mat\\research papers\\New folder\\New folder\\20200507_Cardiology_HCP_Data.xlsx')\n",
    "Embedding_Column_Name = [\"AUTHOR_ID\", \"AUTHOR_NAME\",\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Location Name\",\"Gender\",\"Mentioned Authors\",\"Tags\",\"Twitter Reply to\",\"Twitter Retweet of\",\"Thread Entry Type\",\"Snippet\"]\n",
    "Main_DF = df[Embedding_Column_Name]\n",
    "Main_DF = Main_DF.groupby('AUTHOR_ID',as_index= False)[\"AUTHOR_NAME\",\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Location Name\",\"Gender\",\"Mentioned Authors\",\"Tags\",\"Twitter Reply to\",\"Twitter Retweet of\",\"Thread Entry Type\",\"Snippet\"].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Making a cleaner dataframe ####\n",
    "for columName in Embedding_Column_Name[1:]:\n",
    "    for _,x in Main_DF.iterrows():\n",
    "        x[columName] = list(set(x[columName]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting additional user information from the data:\n",
    "Main_DF['TweetReTweetUser'] = [GetTweetRetweetfromRow(x) for _,x in Main_DF.iterrows()]\n",
    "Main_DF['AllMentioneduser'] = [AllMentionedUser(x) for _,x in Main_DF.iterrows()]  \n",
    "Main_DF['AllConnection'] = [AllConnections(x) for _,x in Main_DF.iterrows()] \n",
    "for _,x in Main_DF.iterrows():\n",
    "    All_OccuranceList.extend(x['AllConnection'])\n",
    "    BigList.append(x['AllConnection'])\n",
    "All_User = list(All_User_for_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Graph based approach main coding ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### declaring some global variables\n",
    "G = nx.Graph()\n",
    "p = 1\n",
    "q = 1 # p,q are the probabilties\n",
    "alias_nodes = {}\n",
    "alias_edges = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main part that generates the walk\n",
    "def node2vec_walk(walk_length, start_node):\n",
    "    walk = [start_node]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        cur_nbrs = sorted(G.neighbors(cur))\n",
    "        if len(cur_nbrs) > 0:\n",
    "            if len(walk) == 1:\n",
    "                walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "            else:\n",
    "                prev = walk[-2]\n",
    "                next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],alias_edges[(prev, cur)][1])]\n",
    "                walk.append(next)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_walks(num_walks, walk_length):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    print ('Walk iteration:')\n",
    "    for walk_iter in range(num_walks):\n",
    "        print (str(walk_iter+1), '/', str(num_walks))\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give probabilty to edges on basis of p and q ( p is for dfs and q is for bfs)\n",
    "def get_alias_edge(src, dst):\n",
    "    unnormalized_probs = []\n",
    "    for dst_nbr in sorted(G.neighbors(dst)):\n",
    "        if dst_nbr == src:\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "        elif G.has_edge(dst_nbr, src):\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "        else:\n",
    "            unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "    return alias_setup(normalized_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "    return J, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set values to nodes and edges\n",
    "def preprocess_transition_probs():\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "        alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "    for edge in G.edges():\n",
    "        alias_edges[edge] = get_alias_edge(edge[0], edge[1])\n",
    "        alias_edges[(edge[1], edge[0])] = get_alias_edge(edge[1], edge[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_draw(J, q):\n",
    "    K = len(J)\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read table obtain data and prepare the Graph ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTargetConnection(authorid):\n",
    "    totalList= []\n",
    "    for lis in BigList:\n",
    "        if(lis.count(authorid) > 0):\n",
    "            totalList.extend(lis)\n",
    "    k = list(filter((authorid).__ne__, totalList))\n",
    "    return k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHAHBAZ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\SHAHBAZ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Not all columns are needed to create the Graphs, a source node and target nodes are needed \n",
    "New_DF = Main_DF[[\"AUTHOR_ID\",\"AllConnection\"]]\n",
    "New_DF['Target_Connection'] = \"\"\n",
    "New_DF['Target_Connection'] = [CreateTargetConnection(x['AUTHOR_ID']) for _,x in New_DF.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUTHOR_ID</th>\n",
       "      <th>AllConnection</th>\n",
       "      <th>Target_Connection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0moba</td>\n",
       "      <td>[victorr_ugo, docneto, 0moba]</td>\n",
       "      <td>[victorr_ugo, docneto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600ronnell</td>\n",
       "      <td>[1600ronnell, fox5dc]</td>\n",
       "      <td>[fox5dc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1963Kelli</td>\n",
       "      <td>[dailysoundnfury, 1963Kelli, koolkaryn]</td>\n",
       "      <td>[dailysoundnfury, koolkaryn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1EyeSmart1</td>\n",
       "      <td>[1EyeSmart1, BusyBrain_Very]</td>\n",
       "      <td>[BusyBrain_Very]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1HopelesslyHope</td>\n",
       "      <td>[CNN, 1HopelesslyHope]</td>\n",
       "      <td>[CNN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AUTHOR_ID                            AllConnection  \\\n",
       "0            0moba            [victorr_ugo, docneto, 0moba]   \n",
       "1      1600ronnell                    [1600ronnell, fox5dc]   \n",
       "2        1963Kelli  [dailysoundnfury, 1963Kelli, koolkaryn]   \n",
       "3       1EyeSmart1             [1EyeSmart1, BusyBrain_Very]   \n",
       "4  1HopelesslyHope                   [CNN, 1HopelesslyHope]   \n",
       "\n",
       "              Target_Connection  \n",
       "0        [victorr_ugo, docneto]  \n",
       "1                      [fox5dc]  \n",
       "2  [dailysoundnfury, koolkaryn]  \n",
       "3              [BusyBrain_Very]  \n",
       "4                         [CNN]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for user in All_User:\n",
    "    G.add_node(user)\n",
    "    \n",
    "for _,x in New_DF.iterrows():\n",
    "    source = x['AUTHOR_ID']\n",
    "    lis = x['Target_Connection']\n",
    "    \n",
    "    for target in lis:\n",
    "        G.add_edge(source, target)\n",
    "        G[source][target]['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alias_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_transition_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk iteration:\n",
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "10 / 10\n"
     ]
    }
   ],
   "source": [
    "walks = simulate_walks(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21445921, 22066600)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#skip gram model to generate embedding from walks\n",
    "model = Word2Vec(window = 10, sg = 1, size = 100, hs = 0,negative = 10,alpha=0.03, min_alpha=0.0007,seed = 14)\n",
    "#size of 100,200 and 300 dimensions were tested and out of which 100 dimension performed best in MultiView\n",
    "model.build_vocab(walks, progress_per=2)\n",
    "model.train(walks, total_examples = model.corpus_count, epochs=20, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHAHBAZ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\SHAHBAZ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "New_DF['NetworkEmbedding'] = [model[x['AUTHOR_ID']] for _,x in New_DF.iterrows()]\n",
    "Node2VecMat = np.zeros(100)\n",
    "for _,x in New_DF.iterrows():\n",
    "    Node2VecMat = np.vstack((Node2VecMat,x['NetworkEmbedding']))\n",
    "Node2VecMat = Node2VecMat[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Savefilepath = \"C:/Users/SHAHBAZ/ZS UserEmbedding/AutoEncoderInputUserMatrix/\"\n",
    "fileName = Savefilepath + \"Node2Vec_100_Matrix.txt\"\n",
    "np.savetxt(fileName,Node2VecMat,fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
