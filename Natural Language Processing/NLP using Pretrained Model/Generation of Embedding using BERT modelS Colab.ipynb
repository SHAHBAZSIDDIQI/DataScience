{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generation of Embedding using BERT model_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "naZMTJrvdP6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The purpose of the notebook is to generate embeddings using the pre trained BERT model.\n",
        "#AutoEncoders utilised for merging and other purpose are defined and trained in this notebook only.\n",
        "#We have utilised 3 BERT model here, BioBERT, SciBERT and Clinical BERT\n",
        "#Towards end of the notebook we have defined code for merging of non-linear graph representation of users with text based representation.\n",
        "#Note: none of the embedding evaluation occurs in the notebook, rather a matrix is generated and saved locally to carry out at local machine. \n",
        "#Reason being colab only supports 8 hours per day usage and if disconnected the variables loses their value, retraining encoder is time consuming."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fLfIugVWejx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular')\n",
        "from scipy import spatial\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R79GrooeO71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/20200507_Cardiology_HCP_Data.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrK9ff51fb2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Embedding_Column_Name = [\"AUTHOR_ID\", \"TRANS_AUTHOR_BIO\",\"Account Type\",\"Snippet\"]\n",
        "Main_DF = df[Embedding_Column_Name]\n",
        "Main_DF = Main_DF.groupby('AUTHOR_ID',as_index= False)[\"TRANS_AUTHOR_BIO\",\"Account Type\",\"Snippet\"].agg(lambda x: list(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mFyK91Ff9Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Making a cleaner dataframe ####\n",
        "for columName in Embedding_Column_Name[1:]:\n",
        "    for _,x in Main_DF.iterrows():\n",
        "        x[columName] = list(set(x[columName]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhFeUYqIgH-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_UsrDoc_df = Main_DF.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn0_Q8KngWrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ Cleaning the tweets #################\n",
        "# lowercase\n",
        "# remove http\n",
        "# remove punctuations\n",
        "# remove @\n",
        "# remove #\n",
        "# remove stopwords or lemmatize\n",
        "\n",
        "def RemoveHTTP(tweet):\n",
        "  clean_tweet = re.match('(.*?)https.*?\\s?(.*?)',str(tweet))\n",
        "  if(clean_tweet):\n",
        "    return clean_tweet.group(1)\n",
        "  else:\n",
        "    return tweet\n",
        "\n",
        "def removePunc(inputstr):\n",
        "  for x in inputstr.lower():\n",
        "    if x in punctuations:\n",
        "      inputstr = inputstr.replace(x, \"\")\n",
        "  return inputstr\n",
        "\n",
        "def Clean(tweet):\n",
        "  clean_tweet = re.sub('@','',str(tweet))\n",
        "  clean_tweet = re.sub('#','',str(clean_tweet))\n",
        "  clean_tweet = re.sub('RT','',str(clean_tweet))\n",
        "  clean_tweet = removePunc(clean_tweet)\n",
        "  return clean_tweet\n",
        "  #tweet = tweet.strip()\n",
        "\n",
        "def removeStopWordsandLemmatize(inputstr):\n",
        "  token = word_tokenize(inputstr)\n",
        "  result = [i for i in token if not i in stop_words]\n",
        "  result = [lemmatizer.lemmatize(i) for i in result]\n",
        "  return ' '.join(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbKlaTnjz60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install biobert-embedding==0.1.2\n",
        "from biobert_embedding.embedding import BiobertEmbedding\n",
        "biobert_model = BiobertEmbedding()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU7caPEZi5zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################### Creating Training data and training AutoEncoder 1536 to 768 ##################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6PcTAx0lHId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CompleteCleaning(tweet):\n",
        "  clean_tweet = RemoveHTTP(tweet)\n",
        "  clean_tweet = Clean(clean_tweet)\n",
        "  clean_tweet = removeStopWordsandLemmatize(clean_tweet)\n",
        "  return clean_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khXb1BjbjuMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = Main_UsrDoc_df.tail(2500) # since we will be evaluating with top 1500 user data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzRAhO-anC3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BuildCorpus = []\n",
        "for _,x in train_df.iterrows():\n",
        "  BuildCorpus.append(x['Snippet'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rwDI74no0c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BuildCorpus_New = []\n",
        "for tweetList in BuildCorpus:\n",
        "  for tweet in tweetList:\n",
        "    print(tweet)\n",
        "    BuildCorpus_New.append(tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P6IarT8qRhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(BuildCorpus_New)):\n",
        "  BuildCorpus_New[i] = CompleteCleaning(BuildCorpus_New[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjfkdiNxxFCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TrainMat1536 = np.zeros(1536)\n",
        "for i in range(0,len(BuildCorpus_New),2):\n",
        "  vec = []\n",
        "  j = i + 1\n",
        "  emb_i = biobert_model.sentence_vector(BuildCorpus_New[i])\n",
        "  emb_j = biobert_model.sentence_vector(BuildCorpus_New[j])\n",
        "  vec = np.concatenate((vec,emb_i), axis=None)\n",
        "  vec = np.concatenate((vec,emb_j), axis=None)\n",
        "  TrainMat1536 = np.vstack((TrainMat1536,vec))\n",
        "  print(\"Epoch \" + str(i) + \" / \" + str(len(BuildCorpus_New)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn3jgpvE4ZrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TrainMat1536.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEsFTSVaxaqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############### creating Auto Encoder for 1536 to 768 ################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW_-tW7CLNGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(TrainMat1536)\n",
        "training_data, test_data = TrainMat1536[:1900,:],TrainMat1536[1900:,:]   #train and test data for AutoEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i4MlKgW4kjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,Flatten,Input\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1ATkJ1iYhhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining AutoEncoder\n",
        "encoding_dims = 768\n",
        "input_dim = Input(shape = (1536,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(1536, activation='tanh')(encoded)\n",
        "autoencoder_1536to768 = Model(input_dim,decoded)\n",
        "\n",
        "encoder_1536to768 = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder_1536to768.layers[-1]\n",
        "# create the decoder model\n",
        "decoder_1536to768 = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder_1536to768.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnt8Wr2nYvDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder_1536to768.fit(training_data, training_data,\n",
        "                epochs=500,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_data, test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZlTMNP34rT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['CombinedEmbedding'] = \"\"  # Adding additional column to store the generated vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY1q82ox6g3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function that accepts list of tweets and return an embedding using the userdoc approach only -- AutoEncoders are mainly used in user doc approach\n",
        "# We use Bio BERT to generate sentence embedding\n",
        "def generateSnippetEmbeding(tweetList):\n",
        "  tweetEmbed = []\n",
        "  for tweet in tweetList:\n",
        "    emb = biobert_model.sentence_vector(CompleteCleaning(tweet))\n",
        "    tweetEmbed.append(emb)\n",
        "  mainvec = tweetEmbed[0]\n",
        "  if(len(tweetList) > 1):  \n",
        "    for i in range(1,len(tweetEmbed)):\n",
        "      vec = np.concatenate((mainvec,tweetEmbed[i]), axis = None)\n",
        "      fmax = vec\n",
        "      fmax = np.vstack((fmax,vec))\n",
        "      emb = encoder_1536to768.predict(fmax)\n",
        "      mainvec = emb[0]\n",
        "  return np.asarray(mainvec)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peUyYk5c-sNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['SnippetEmbedding'] = \"\" # adding column to accept tweet embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qev0Tv_EEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['SnippetEmbedding'] = [generateSnippetEmbeding(x['Snippet']) for _,x in train_df.iterrows()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNnPKZyc-7TU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['TranAuthorEmb'] = \"\" #adding column to accept user about info embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPL2f6LFkKLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJM7bJH0B_1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 1\n",
        "for _,x in train_df.iterrows():\n",
        "  x['SnippetEmbedding'] = generateSnippetEmbeding(x['Snippet'])\n",
        "  print(\"Epoch \" + str(count) + \" / 4000\")\n",
        "  count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaTqY8wpAxVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 1\n",
        "for _,x in train_df.iterrows():\n",
        "  x['TranAuthorEmb'] = np.asarray(biobert_model.sentence_vector(CompleteCleaning(x['TRANS_AUTHOR_BIO'])))\n",
        "  print(\"Epoch \"+str(count) + \" / 2500\")\n",
        "  count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djx5iJOpFcbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['TranAuthorEmb'][4000].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uTAT9fcFeYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ Train Matrix for Combining the Embedding that is 1536 to 300 dimension ###########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwPajnT5HN85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Mat1536to300 = np.zeros(1536)\n",
        "for _,x in train_df.iterrows():\n",
        "  vec = []\n",
        "  vec = np.concatenate((vec,x['SnippetEmbedding']), axis = None)\n",
        "  vec = np.concatenate((vec,x['TranAuthorEmb']), axis= None)\n",
        "  Mat1536to300 = np.vstack((Mat1536to300,vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fg7OyJJHypG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Mat1536to300 = Mat1536to300[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NU11X10-IAXH",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(Mat1536to300)\n",
        "training_data, test_data = Mat1536to300[:1900,:],Mat1536to300[1900:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3B6Ecp_IAXS",
        "colab": {}
      },
      "source": [
        "encoding_dims = 300\n",
        "input_dim = Input(shape = (1536,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(1536, activation='tanh')(encoded)\n",
        "autoencoder_1536to300 = Model(input_dim,decoded)\n",
        "\n",
        "encoder_1536to300 = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder_1536to300.layers[-1]\n",
        "# create the decoder model\n",
        "decoder_1536to300 = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder_1536to300.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zddmz_uEIAXX",
        "colab": {}
      },
      "source": [
        "autoencoder_1536to300.fit(training_data, training_data,\n",
        "                epochs=500,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_data, test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvFStvJbIS2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the trained Auto Encoders for later use\n",
        "from google.colab import files\n",
        "autoencoder_1536to300.save('/content/drive/My Drive/Colab Notebooks/tweets/autoencoder_1536to300.h5')\n",
        "decoder_1536to300.save('/content/drive/My Drive/Colab Notebooks/tweets/decoder_1536to300.h5')\n",
        "encoder_1536to300.save('/content/drive/My Drive/Colab Notebooks/tweets/encoder_1536to300.h5')\n",
        "autoencoder_1536to768.save('/content/drive/My Drive/Colab Notebooks/tweets/autoencoder_1536to768.h5')\n",
        "encoder_1536to768.save('/content/drive/My Drive/Colab Notebooks/tweets/encoder_1536to768.h5')\n",
        "decoder_1536to768.save('/content/drive/My Drive/Colab Notebooks/tweets/decoder_1536to768.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEZgFDvVJnQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function accepts a user data and generate embedding using the autoencoder and bert trained model, Finally the resultant vector of 1536 is \n",
        "# reduced to form a reduced version of user embedding\n",
        "def Generate_CombinedEmbedding(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding(rowd['Snippet'])\n",
        "  transAuthEmb = np.asarray(biobert_model.sentence_vector(CompleteCleaning(rowd['TRANS_AUTHOR_BIO'])))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  resultantvec = np.zeros(1536)\n",
        "  resultantvec = np.vstack((resultantvec,vec))\n",
        "  emb = encoder_1536to300.predict(resultantvec)\n",
        "  return np.asarray(emb[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkQoWoeMK9ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Main_UsrDoc_df['CombinedEmbedding'] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_wE4CA4LGb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.iterrows():\n",
        "  x['CombinedEmbedding'] = Generate_CombinedEmbedding(x)\n",
        "  print(\"Epoch \"+str(count) + \" / \" + str(Main_UsrDoc_df.shape[0]))\n",
        "  count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h26NXBCNNiD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating matrix for reduced dimension to be evaluated locally\n",
        "UserDocTop900Emb_BioBert = np.zeros(300)\n",
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.head(900).iterrows():\n",
        "  UserDocTop900Emb_BioBert = np.vstack((UserDocTop900Emb_BioBert,np.asarray(x['CombinedEmbedding'])))\n",
        "  print(\"Epoch \" + str(count) + \" / 900\")\n",
        "  count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQauQLMJOPD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UserDocTop900Emb_BioBert.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7AD63xCOYZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UserDocTop900Emb_BioBert = UserDocTop900Emb_BioBert[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-8s9wHrOa8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/BioBERT_UserDoc_Combined_Matrix.txt'\n",
        "np.savetxt(fileName,UserDocTop900Emb_BioBert,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E146iRxoOuRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generate_1536Embedding(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding(rowd['Snippet'])\n",
        "  transAuthEmb = np.asarray(biobert_model.sentence_vector(CompleteCleaning(rowd['TRANS_AUTHOR_BIO'])))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  return np.asarray(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32JZ45cxdrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating matrix for full dimension to be evaluated locally\n",
        "TransMat1536_full = np.zeros(1536)\n",
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.head(1000).iterrows():\n",
        "  TransMat1536_full = np.vstack((TransMat1536_full,Generate_1536Embedding(x)))\n",
        "  print(\"Epoch \"+ str(count) + \" / 1000\")\n",
        "  count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccfo6UKh4FLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TransMat1536_full.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crmvwflc4IHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TransMat1536_full = TransMat1536_full[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJTZSIPS693M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/BioBERT_UserDoc_Full_Matrix.txt'\n",
        "np.savetxt(fileName,TransMat1536_full,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c98FMoya7E2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################# Trying Sci BERT ########################## "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQbi5SoO7Rvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tOEuG_XPeivD",
        "colab": {}
      },
      "source": [
        "def GenerateEmbSCiBERT(text):\n",
        "  input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
        "  output = model(input_ids)\n",
        "  d1 = output[0].mean(1).detach().numpy()\n",
        "  return d1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BfCXu7YX8j8t",
        "colab": {}
      },
      "source": [
        "def generateSnippetEmbeding_Sci(tweetList):\n",
        "  tweetEmbed = []\n",
        "  for tweet in tweetList:\n",
        "    emb = GenerateEmbSCiBERT(CompleteCleaning(tweet))\n",
        "    tweetEmbed.append(emb)\n",
        "  mainvec = tweetEmbed[0]\n",
        "  if(len(tweetList) > 1):  \n",
        "    for i in range(1,len(tweetEmbed)):\n",
        "      vec = np.concatenate((mainvec,tweetEmbed[i]), axis = None)\n",
        "      fmax = vec\n",
        "      fmax = np.vstack((fmax,vec))\n",
        "      emb = encoder_1536to768.predict(fmax)\n",
        "      mainvec = emb[0]\n",
        "  return np.asarray(mainvec)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n68sPQK08j8x",
        "colab": {}
      },
      "source": [
        "def Generate_CombinedEmbedding_Sci(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding_Sci(rowd['Snippet'])\n",
        "  transAuthEmb = GenerateEmbSCiBERT(CompleteCleaning(rowd['TRANS_AUTHOR_BIO']))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  resultantvec = np.zeros(1536)\n",
        "  resultantvec = np.vstack((resultantvec,vec))\n",
        "  emb = encoder_1536to300.predict(resultantvec)\n",
        "  return np.asarray(emb[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-C6Mv-78tIT",
        "colab": {}
      },
      "source": [
        "def Generate_1536Embedding_Sci(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding_Sci(rowd['Snippet'])\n",
        "  transAuthEmb = GenerateEmbSCiBERT(CompleteCleaning(rowd['TRANS_AUTHOR_BIO']))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  return np.asarray(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVC0Ix1D9bIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SciBertMat_300 = np.zeros(300)\n",
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.head(1000).iterrows():\n",
        "    SciBertMat_300 = np.vstack((SciBertMat_300,Generate_CombinedEmbedding_Sci(x)))\n",
        "    print(\"Epoch \"+ str(count) + \" / 1000\")\n",
        "    count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GghSOrvG-vVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SciBertMat_300 = SciBertMat_300[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CK_yTOPAldD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/SciBERT_UserDoc_Combined_Matrix.txt'\n",
        "np.savetxt(fileName,SciBertMat_300,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vxpf9y3xA6fz",
        "colab": {}
      },
      "source": [
        "SciBertMat_1536 = np.zeros(1536)\n",
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.head(1000).iterrows():\n",
        "    SciBertMat_1536 = np.vstack((SciBertMat_1536,Generate_1536Embedding_Sci(x)))\n",
        "    print(\"Epoch \"+ str(count) + \" / 1000\")\n",
        "    count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gw50yjfAxiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SciBertMat_1536 = SciBertMat_1536[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD32E1T1C7ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/SciBERT_UserDoc_1536_Matrix.txt'\n",
        "np.savetxt(fileName,SciBertMat_1536,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB16goGGDEd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################# Clinical BERT #####################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5M3uUVNvFADf",
        "colab": {}
      },
      "source": [
        "tokenizer_clinical = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model_clinical = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VxxXkYk_FADj",
        "colab": {}
      },
      "source": [
        "def GenerateEmbClinicalBERT(text):\n",
        "  input_ids = torch.tensor(tokenizer_clinical.encode(text)).unsqueeze(0)\n",
        "  output = model_clinical(input_ids)\n",
        "  d1 = output[0].mean(1).detach().numpy()\n",
        "  return d1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3IUanGpSFADl",
        "colab": {}
      },
      "source": [
        "def generateSnippetEmbeding_Clinical(tweetList):\n",
        "  tweetEmbed = []\n",
        "  for tweet in tweetList:\n",
        "    emb = GenerateEmbClinicalBERT(CompleteCleaning(tweet))\n",
        "    tweetEmbed.append(emb)\n",
        "  mainvec = tweetEmbed[0]\n",
        "  if(len(tweetList) > 1):  \n",
        "    for i in range(1,len(tweetEmbed)):\n",
        "      vec = np.concatenate((mainvec,tweetEmbed[i]), axis = None)\n",
        "      fmax = vec\n",
        "      fmax = np.vstack((fmax,vec))\n",
        "      emb = encoder_1536to768.predict(fmax)\n",
        "      mainvec = emb[0]\n",
        "  return np.asarray(mainvec)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuHcQ76EFADo",
        "colab": {}
      },
      "source": [
        "def Generate_CombinedEmbedding_Clinical(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding_Clinical(rowd['Snippet'])\n",
        "  transAuthEmb = GenerateEmbClinicalBERT(CompleteCleaning(rowd['TRANS_AUTHOR_BIO']))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  resultantvec = np.zeros(1536)\n",
        "  resultantvec = np.vstack((resultantvec,vec))\n",
        "  emb = encoder_1536to300.predict(resultantvec)\n",
        "  return np.asarray(emb[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qovudzflFADs",
        "colab": {}
      },
      "source": [
        "def Generate_1536Embedding_Clinical(rowd):\n",
        "  snipetEmb = generateSnippetEmbeding_Clinical(rowd['Snippet'])\n",
        "  transAuthEmb = GenerateEmbClinicalBERT(CompleteCleaning(rowd['TRANS_AUTHOR_BIO']))\n",
        "  vec = []\n",
        "  vec = np.concatenate((snipetEmb,transAuthEmb),axis = None)\n",
        "  return np.asarray(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-cTmBHUFADu",
        "colab": {}
      },
      "source": [
        "ClinicalBertMat_300 = np.zeros(300)\n",
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.head(1000).iterrows():\n",
        "    ClinicalBertMat_300 = np.vstack((ClinicalBertMat_300,Generate_CombinedEmbedding_Clinical(x)))\n",
        "    print(\"Epoch \"+ str(count) + \" / 1000\")\n",
        "    count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSvFXQbWFADx",
        "colab": {}
      },
      "source": [
        "ClinicalBertMat_300 = ClinicalBertMat_300[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ejUPWxU6FAD0",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/ClinicalBERT_UserDoc_Combined_Matrix.txt'\n",
        "np.savetxt(fileName,ClinicalBertMat_300,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TvOX2DAaFAD2",
        "colab": {}
      },
      "source": [
        "ClinicalBertMat_1536 = np.zeros(1536)\n",
        "count = 1\n",
        "for _,x in Main_UsrDoc_df.head(1000).iterrows():\n",
        "    ClinicalBertMat_1536 = np.vstack((ClinicalBertMat_1536,Generate_1536Embedding_Clinical(x)))\n",
        "    print(\"Epoch \"+ str(count) + \" / 1000\")\n",
        "    count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4VbfK_SCFAD4",
        "colab": {}
      },
      "source": [
        "ClinicalBertMat_1536 = ClinicalBertMat_1536[1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7vX99ZdFAD8",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/ClinicalBERT_UserDoc_1536_Matrix.txt'\n",
        "np.savetxt(fileName,ClinicalBertMat_1536,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J-6DjKW26do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################### MultiView Approach using Deep Walk network representation ################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xle8i25mjxXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,Flatten,Input\n",
        "from keras.layers import BatchNormalization,Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXeW_gfhint-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the matrix of user representation that contain 1536 vector of tweet and user about info data representation and 100 dimension of deep walk data representation\n",
        "# The purpose if these matrix is to train and define an AutoEncoder to generate low dimension reduced vector.\n",
        "MultiView_1636_mat = np.loadtxt('/content/drive/My Drive/Colab Notebooks/Embeddings/MultiView_1636_Matrix.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i-nEAg8jbco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main,X_train,X_test = MultiView_1636_mat[:1000],MultiView_1636_mat[1000:3500],MultiView_1636_mat[3500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar14TZm02wJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split into training and validation for autoencoder\n",
        "np.random.shuffle(X_train)\n",
        "np.random.shuffle(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkBP0IJw20o-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding_dims = 550\n",
        "input_dim = Input(shape = (1636,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(1636, activation='tanh')(encoded)\n",
        "autoencoder = Model(input_dim,decoded)\n",
        "\n",
        "encoder = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyF_yN5JJXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=500,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZcqWK4CjiHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_1000 = encoder.predict(main)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9MK7tNYm3Dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_1000.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SYFpNVSlsKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/MultiView_DeepWalk_1636to550_Matrix.txt'\n",
        "np.savetxt(fileName,result_1000,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LiGEejCluX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ trying to improve benchmark #############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLOgYaxAvOzf",
        "colab": {}
      },
      "source": [
        "# Loading the matrix of a combination of reduced embedding of 300 dimension of user representation from tweet and about info and 100 dimension if deep walk representation\n",
        "# The purpose of the matrix is to define and generate a reduced version of the 400 dimension data\n",
        "MultiView_400_mat = np.loadtxt('/content/drive/My Drive/Colab Notebooks/Embeddings/MultiView_Biobert_reduced_400.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DjmYb1zCvOzj",
        "colab": {}
      },
      "source": [
        "main,X_train,X_test = MultiView_400_mat[:1000],MultiView_400_mat[1000:3500],MultiView_400_mat[3500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "36cxiJyhvOzm",
        "colab": {}
      },
      "source": [
        "#split into training and validation for autoencoder\n",
        "np.random.shuffle(X_train)\n",
        "np.random.shuffle(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xFNRGUAfvOzp",
        "colab": {}
      },
      "source": [
        "encoding_dims = 300\n",
        "input_dim = Input(shape = (400,))\n",
        "encoded = Dense(encoding_dims, activation='relu')(input_dim)\n",
        "decoded = Dense(400, activation='tanh')(encoded)\n",
        "autoencoder = Model(input_dim,decoded)\n",
        "\n",
        "encoder = Model(input_dim,encoded)\n",
        "\n",
        "encoded_input = Input(shape=(encoding_dims,))\n",
        "# retrieve the last layer of the autoencoder model\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "# create the decoder model\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "autoencoder.compile(optimizer='adadelta', loss=mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IQw8DbGvvOzr",
        "colab": {}
      },
      "source": [
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=1000,\n",
        "                batch_size=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDdA1yqcvOzt",
        "colab": {}
      },
      "source": [
        "result_1000 = encoder.predict(main)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VzufLeJmvOzv",
        "colab": {}
      },
      "source": [
        "result_1000.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oShE296rvOzx",
        "colab": {}
      },
      "source": [
        "fileName = '/content/drive/My Drive/Colab Notebooks/MultiView_DeepWalk_reduced_biobert_400to300_Matrix.txt'\n",
        "np.savetxt(fileName,result_1000,fmt='%.8f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcsWzFJUwGqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}