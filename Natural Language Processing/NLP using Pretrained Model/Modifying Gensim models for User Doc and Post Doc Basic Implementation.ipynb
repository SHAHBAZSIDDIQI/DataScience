{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Motive of this notebook is to prepare data in order to modify Gensim based paragraph2vec and Document2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using public tweets \n",
    "F = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilteredData_Main = F[F.columns[4:6]]   #Filtering out useful columns from the datasheet\n",
    "FilteredData_Main.columns = [\"UserName\",\"Tweet\"]\n",
    "FilteredData_Main = FilteredData_Main[FilteredData_Main['UserName'].map(FilteredData_Main['UserName'].value_counts()) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating corpus to train gensim model for a user doc approach, \n",
    "##Since User Doc is a individual tweet based approach we limit the users on basis of tweets\n",
    "CorpusForUserDoc = F[F.columns[4:6]]        \n",
    "CorpusForUserDoc.columns = [\"UserName\",\"Tweet\"]\n",
    "CorpusForUserDoc = CorpusForUserDoc[CorpusForUserDoc['UserName'].map(CorpusForUserDoc['UserName'].value_counts()) < 20]\n",
    "CorpusForUserDoc = CorpusForUserDoc[CorpusForUserDoc['UserName'].map(CorpusForUserDoc['UserName'].value_counts()) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDocCorp_df = pd.DataFrame({'author' : CorpusForUserDoc['UserName'].unique()})\n",
    "userDocCorp_df['alltweets'] = [list(set(CorpusForUserDoc['Tweet'].loc[CorpusForUserDoc['UserName'] == x['author']])) for _,x in userDocCorp_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gethashtags(tweetlist):\n",
    "    l = []\n",
    "    for tweet in tweetlist:\n",
    "        l.extend(re.findall(r\"#(\\w+)\",str(tweet)))\n",
    "    return list(set(l))\n",
    "def getmentionedUsers(tweetlist):\n",
    "    l = []\n",
    "    for tweet in tweetlist:\n",
    "        l.extend(re.findall(r\"@(\\w+)\",str(tweet)))\n",
    "    return list(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessRemoveURL(tweetlist):\n",
    "    l = []\n",
    "    for tweet in tweetlist:    \n",
    "        clean_tweet = re.match('(.*?)http.*?\\s?(.*?)',str(tweet))\n",
    "        if(clean_tweet):\n",
    "            l.append(clean_tweet.group(1))\n",
    "        else:\n",
    "            l.append(tweet)\n",
    "        #l.append(re.sub(r\"http:\\+\",\"\",str(tweet)))\n",
    "    return l\n",
    "\n",
    "def removeUserMentioned(tweetlist):\n",
    "    l = []\n",
    "    for tweet in tweetlist:\n",
    "        l.append(re.sub('@[^\\s]+','',str(tweet)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing userCrop data\n",
    "userDocCorp_df['hashtags'] = [gethashtags(x['alltweets']) for _,x in userDocCorp_df.iterrows()]\n",
    "userDocCorp_df['selectedtweets'] = [preprocessRemoveURL(x['alltweets']) for _,x in userDocCorp_df.iterrows()]\n",
    "userDocCorp_df['MentionedUsers'] = [getmentionedUsers(x['alltweets']) for _,x in userDocCorp_df.iterrows()]\n",
    "userDocCorp_df['selectedtweets'] = [removeUserMentioned(x['selectedtweets']) for _,x in userDocCorp_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and modifying data to modify Post-Doc approach, using Gensim Doc2vec Model\n",
    "corpus_totrainmodel = []\n",
    "for _,x in FilteredData_Main.iterrows():\n",
    "    tweet = x[\"Tweet\"]\n",
    "    clean_tweet = re.match('(.*?)http.*?\\s?(.*?)',str(tweet))\n",
    "    if(clean_tweet):\n",
    "        tweet = clean_tweet.group(1)\n",
    "    else:\n",
    "        pass\n",
    "    tweet = re.sub('@[^\\s]+','',str(tweet))\n",
    "    corpus_totrainmodel.append(tweet)\n",
    "corpus = list(set(corpus_totrainmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words = word_tokenize(_d.lower()), tags = [str(i)]) for i,_d in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 100\n",
    "vec_size = 300  # also evaluated against 200,400 and 500,performed best with 300-dimension\n",
    "alpha = 0.025\n",
    "model = Doc2Vec(vector_size = vec_size, alpha = alpha,min_alpha = 0.00025,min_count = 1,dm = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0\n",
      "Model Saved\n",
      "iteration1\n",
      "iteration2\n",
      "iteration3\n",
      "iteration4\n",
      "iteration5\n",
      "iteration6\n",
      "iteration7\n",
      "iteration8\n",
      "iteration9\n",
      "iteration10\n",
      "iteration11\n",
      "iteration12\n",
      "iteration13\n",
      "iteration14\n",
      "iteration15\n",
      "iteration16\n",
      "iteration17\n",
      "iteration18\n",
      "iteration19\n",
      "iteration20\n",
      "iteration21\n",
      "iteration22\n",
      "iteration23\n",
      "iteration24\n",
      "iteration25\n",
      "iteration26\n",
      "iteration27\n",
      "iteration28\n",
      "iteration29\n",
      "iteration30\n",
      "iteration31\n",
      "iteration32\n",
      "iteration33\n",
      "iteration34\n",
      "iteration35\n",
      "iteration36\n",
      "iteration37\n",
      "iteration38\n",
      "iteration39\n",
      "iteration40\n",
      "iteration41\n",
      "iteration42\n",
      "iteration43\n",
      "iteration44\n",
      "iteration45\n",
      "iteration46\n",
      "iteration47\n",
      "iteration48\n",
      "iteration49\n",
      "iteration50\n",
      "Model Saved\n",
      "iteration51\n",
      "iteration52\n",
      "iteration53\n",
      "iteration54\n",
      "iteration55\n",
      "iteration56\n",
      "iteration57\n",
      "iteration58\n",
      "iteration59\n",
      "iteration60\n",
      "iteration61\n",
      "iteration62\n",
      "iteration63\n",
      "iteration64\n",
      "iteration65\n",
      "iteration66\n",
      "iteration67\n",
      "iteration68\n",
      "iteration69\n",
      "iteration70\n",
      "iteration71\n",
      "iteration72\n",
      "iteration73\n",
      "iteration74\n",
      "iteration75\n",
      "iteration76\n",
      "iteration77\n",
      "iteration78\n",
      "iteration79\n",
      "iteration80\n",
      "iteration81\n",
      "iteration82\n",
      "iteration83\n",
      "iteration84\n",
      "iteration85\n",
      "iteration86\n",
      "iteration87\n",
      "iteration88\n",
      "iteration89\n",
      "iteration90\n",
      "iteration91\n",
      "iteration92\n",
      "iteration93\n",
      "iteration94\n",
      "iteration95\n",
      "iteration96\n",
      "iteration97\n",
      "iteration98\n",
      "iteration99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    print('iteration{0}'.format(epoch))\n",
    "    model.train(tagged_data, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "    if((epoch % 50) == 0):\n",
    "        model.save(\"d2v_\"+str(epoch)+\"T.model\")\n",
    "        print(\"Model Saved\")\n",
    "model.save(\"d2vFinal.model\")      #Saving the model for later usage \n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Model for Post_doc method\n",
    "model = Doc2Vec.load(\"d2vFinal.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = ' '.join(user_tweet_df['selectedtweets'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = word_tokenize(test_example.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = model.infer_vector(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and modifying data to modify User-Doc Approach\n",
    "corpus_totrainmodel = []\n",
    "for _,x in userDocCorp_df.iterrows():\n",
    "    tweet = x[\"selectedtweets\"]\n",
    "    clean_tweet = re.match('(.*?)http.*?\\s?(.*?)',str(tweet))\n",
    "    if(clean_tweet):\n",
    "        tweet = clean_tweet.group(1)\n",
    "    else:\n",
    "        pass\n",
    "    tweet = re.sub('@[^\\s]+','',str(tweet))\n",
    "    corpus_totrainmodel.append(tweet)\n",
    "corpus = list(set(corpus_totrainmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words = word_tokenize(_d.lower()), tags = [str(i)]) for i,_d in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User2Doc submodel description\n",
    "#Motive is to generate a Matrix where each row consist of representation of user, This Matrix will be passed as a input to AutoEncoder to reduce dimension \n",
    "#For each user 20 tweets are considered and each tweets are passed through gensim model to generate vector of 50 dimensions each\n",
    "#These 50 dimensional vectors are then concatenated to for a vector of 1000 dimension as a vector for each user. \n",
    "max_epoch = 100\n",
    "vec_size = 50       \n",
    "alpha = 0.025\n",
    "model = Doc2Vec(vector_size = vec_size, alpha = alpha,min_alpha = 0.00025,min_count = 1,dm = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0\n",
      "Model Saved for0 epochs\n",
      "iteration1\n",
      "iteration2\n",
      "iteration3\n",
      "iteration4\n",
      "iteration5\n",
      "iteration6\n",
      "iteration7\n",
      "iteration8\n",
      "iteration9\n",
      "iteration10\n",
      "iteration11\n",
      "iteration12\n",
      "iteration13\n",
      "iteration14\n",
      "iteration15\n",
      "iteration16\n",
      "iteration17\n",
      "iteration18\n",
      "iteration19\n",
      "iteration20\n",
      "iteration21\n",
      "iteration22\n",
      "iteration23\n",
      "iteration24\n",
      "iteration25\n",
      "Model Saved for25 epochs\n",
      "iteration26\n",
      "iteration27\n",
      "iteration28\n",
      "iteration29\n",
      "iteration30\n",
      "iteration31\n",
      "iteration32\n",
      "iteration33\n",
      "iteration34\n",
      "iteration35\n",
      "iteration36\n",
      "iteration37\n",
      "iteration38\n",
      "iteration39\n",
      "iteration40\n",
      "iteration41\n",
      "iteration42\n",
      "iteration43\n",
      "iteration44\n",
      "iteration45\n",
      "iteration46\n",
      "iteration47\n",
      "iteration48\n",
      "iteration49\n",
      "iteration50\n",
      "Model Saved for50 epochs\n",
      "iteration51\n",
      "iteration52\n",
      "iteration53\n",
      "iteration54\n",
      "iteration55\n",
      "iteration56\n",
      "iteration57\n",
      "iteration58\n",
      "iteration59\n",
      "iteration60\n",
      "iteration61\n",
      "iteration62\n",
      "iteration63\n",
      "iteration64\n",
      "iteration65\n",
      "iteration66\n",
      "iteration67\n",
      "iteration68\n",
      "iteration69\n",
      "iteration70\n",
      "iteration71\n",
      "iteration72\n",
      "iteration73\n",
      "iteration74\n",
      "iteration75\n",
      "Model Saved for75 epochs\n",
      "iteration76\n",
      "iteration77\n",
      "iteration78\n",
      "iteration79\n",
      "iteration80\n",
      "iteration81\n",
      "iteration82\n",
      "iteration83\n",
      "iteration84\n",
      "iteration85\n",
      "iteration86\n",
      "iteration87\n",
      "iteration88\n",
      "iteration89\n",
      "iteration90\n",
      "iteration91\n",
      "iteration92\n",
      "iteration93\n",
      "iteration94\n",
      "iteration95\n",
      "iteration96\n",
      "iteration97\n",
      "iteration98\n",
      "iteration99\n",
      "Final Model Saved\n"
     ]
    }
   ],
   "source": [
    "#training the tweet2vec model to generate 50 embedding vector for each tweets\n",
    "for epoch in range(max_epoch):\n",
    "    print('iteration{0}'.format(epoch))\n",
    "    model.train(tagged_data, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "    model.alpha -= 0.0002\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "    if((epoch % 25) == 0):\n",
    "        model.save(\"Tweet2Vec\"+str(epoch)+\".model\")\n",
    "        print(\"Model Saved for\" + str(epoch) + \" epochs\")\n",
    "model.save(\"Tweet2Vec_Final.model\")\n",
    "print(\"Final Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
